# 정치 뉴스 논조 수동 검수 가이드라인

## 📋 목차

1. [검수 개요](#검수-개요)
2. [라벨 정의 및 판단 기준](#라벨-정의-및-판단-기준)
3. [검수 프로세스](#검수-프로세스)
4. [구체적 예시](#구체적-예시)
5. [애매한 케이스 처리](#애매한-케이스-처리)
6. [팀 검수 방법](#팀-검수-방법)
7. [FAQ](#faq)

---

## 검수 개요

### 목적
AI 모델(OpenAI GPT-4o-mini, Claude Haiku 4.5) 간 라벨 불일치가 발생한 1,909건의 정치 뉴스 기사에 대해 정확한 논조를 판단합니다.

### 라벨 종류
- **0: 옹호** - 해당 정책이나 인물을 긍정적으로 서술
- **1: 중립** - 객관적 사실 전달, 균형잡힌 시각
- **2: 비판** - 해당 정책이나 인물을 부정적으로 서술

### 검수 대상
- 총 1,909건의 불일치 기사
- 우선순위 1 (74건): OpenAI와 Claude가 정반대로 판단 (옹호↔비판)
- 우선순위 2 (1,835건): 나머지 불일치

---

## 라벨 정의 및 판단 기준

### 🟢 0: 옹호 (Supportive)

#### 정의
특정 정책, 인물, 정당을 **긍정적으로** 묘사하거나 지지하는 논조

#### 판단 기준 체크리스트
- [ ] 긍정적인 형용사/부사 사용 (훌륭한, 성공적인, 효과적인 등)
- [ ] 업적이나 성과를 강조
- [ ] 긍정적인 인용문 중심 배치
- [ ] 비판적 의견 없거나 최소화
- [ ] 호의적인 프레이밍

#### 키워드 예시
✅ "성공적", "효과적", "긍정적", "호평", "박수", "지지", "환영", "높이 평가"

#### 구체적 예시

**예시 1: 명확한 옹호**
```
제목: "○○○ 후보, 경제 공약으로 전문가들 호평"
내용: ○○○ 대선 후보가 발표한 경제 정책이 전문가들로부터
실현 가능하고 효과적이라는 평가를 받았다. 경제학자 ▢▢▢는
"매우 현실적이고 구체적인 방안"이라고 높이 평가했다.
```
→ **라벨: 0 (옹호)**
- "호평", "실현 가능", "효과적", "높이 평가" 등 긍정적 표현
- 전문가의 긍정적 평가 중심

**예시 2: 암묵적 옹호**
```
제목: "△△△ 후보, 시민들과 소통하며 지지 호소"
내용: △△△ 후보가 시민들과 허심탄회한 대화를 나누며
따뜻한 공감 능력을 보여줬다. 많은 시민들이 손을 흔들며
환호했다.
```
→ **라벨: 0 (옹호)**
- "허심탄회", "따뜻한 공감", "환호" 등 긍정적 묘사
- 시민 반응을 긍정적으로 부각

---

### 🔵 1: 중립 (Neutral)

#### 정의
사실만을 객관적으로 전달하거나, 찬반 양쪽 의견을 균형있게 제시하는 논조

#### 판단 기준 체크리스트
- [ ] 사실 위주의 서술 (육하원칙)
- [ ] 감정적/평가적 표현 최소화
- [ ] 찬반 의견을 균형있게 제시
- [ ] 기자의 의견이나 판단 배제
- [ ] 중립적인 동사 사용 (말했다, 밝혔다, 전했다)

#### 키워드 예시
✅ "밝혔다", "전했다", "발표했다", "개최했다", "참석했다", "한편"

#### 구체적 예시

**예시 1: 사실 전달형**
```
제목: "◇◇◇ 후보, 부산에서 유세"
내용: ◇◇◇ 대선 후보가 2일 오후 부산역 광장에서
유세를 열고 시민들에게 지지를 호소했다.
```
→ **라벨: 1 (중립)**
- 순수 사실만 전달
- 평가적 표현 없음

**예시 2: 균형 보도형**
```
제목: "●●● 정책, 전문가들 평가 엇갈려"
내용: ●●● 후보의 경제 정책에 대해 전문가들의 평가가
엇갈렸다. 일부는 "실현 가능성이 높다"고 평가한 반면,
다른 전문가들은 "재원 마련 방안이 불명확하다"고 지적했다.
```
→ **라벨: 1 (중립)**
- 긍정/부정 의견 균형있게 제시
- 기자의 판단 배제

**예시 3: 단순 일정 보도**
```
제목: "□□□ 후보, 오늘 대구·부산 방문"
내용: □□□ 후보가 오늘 대구와 부산을 방문해
지역 주민들과 만난다.
```
→ **라벨: 1 (중립)**
- 일정만 전달
- 어떠한 평가도 없음

---

### 🔴 2: 비판 (Critical)

#### 정의
특정 정책, 인물, 정당을 **부정적으로** 묘사하거나 비판하는 논조

#### 판단 기준 체크리스트
- [ ] 부정적인 형용사/부사 사용 (논란, 비난, 문제, 실패 등)
- [ ] 문제점이나 실패를 강조
- [ ] 비판적 인용문 중심 배치
- [ ] 의혹이나 스캔들 부각
- [ ] 부정적인 프레이밍

#### 키워드 예시
✅ "논란", "비난", "지적", "문제", "실패", "우려", "반발", "비판", "의혹"

#### 구체적 예시

**예시 1: 명확한 비판**
```
제목: "○○○ 후보 공약, 실현 가능성 논란"
내용: ○○○ 후보의 주요 공약에 대해 전문가들이
"재원 마련 방안이 전혀 없다"며 강하게 비판했다.
야당도 "비현실적인 포퓰리즘"이라고 맹비난했다.
```
→ **라벨: 2 (비판)**
- "논란", "비판", "맹비난" 등 부정적 표현
- 전문가/야당의 부정적 평가 중심

**예시 2: 의혹 제기형**
```
제목: "△△△ 후보 측근, 비리 의혹 제기돼"
내용: △△△ 후보의 측근이 부동산 투기 의혹에 휩싸였다.
시민단체는 "명확한 해명이 필요하다"고 촉구했다.
```
→ **라벨: 2 (비판)**
- "의혹", "휩싸였다" 등 부정적 프레이밍
- 문제 상황 부각

**예시 3: 문제점 지적형**
```
제목: "◇◇◇ 후보 유세, 시민 참여 저조"
내용: ◇◇◇ 후보의 유세에 예상보다 적은 수의 시민들만
참석했다. 지지율 하락의 영향으로 풀이된다.
```
→ **라벨: 2 (비판)**
- "저조", "하락" 등 부정적 상황 강조
- 문제점 부각

---

## 검수 프로세스

### 1단계: 기사 읽기

1. **제목** 먼저 읽기
   - 제목의 프레이밍 파악
   - 감정적 표현 확인

2. **내용** 꼼꼼히 읽기
   - 전체 맥락 이해
   - 사용된 어휘와 표현 분석
   - 인용문의 톤 확인

3. **주제(topic)** 참고
   - 기사의 배경 맥락 이해

### 2단계: 양측 라벨 확인

1. **OpenAI 라벨** 확인 (`label_openai`)
2. **Claude 라벨** 확인 (`label_claude`)
3. 두 모델의 판단 이유 추론

### 3단계: 독립적 판단

**중요**: 먼저 AI 라벨을 **보지 않고** 스스로 판단한 후, AI 라벨과 비교하세요.

#### 판단 순서
1. 기사의 **전체적 톤** 파악
2. **주요 메시지** 식별
3. **프레이밍** 분석 (어떻게 이야기를 구성했는가?)
4. 체크리스트로 검증
5. 최종 라벨 결정

### 4단계: 라벨 입력

CSV 파일에서:
1. `final_label` 열에 **0, 1, 2** 중 하나 입력
2. `reviewed` 열에 **TRUE** 입력
3. `reviewer` 열에 **본인 이름** 입력
4. `review_notes` 열에 **판단 근거 간략히** 작성 (선택)

---

## 애매한 케이스 처리

### 케이스 1: 사실 전달이지만 선택적 정보

```
제목: "○○○ 후보, 지지율 1% 하락"
내용: ○○○ 후보의 지지율이 지난주 대비 1%p 하락한
것으로 조사됐다.
```

**판단**:
- 사실만 전달하지만 "하락"이라는 부정적 정보만 선택
- → **라벨: 2 (비판)** (부정적 정보 부각)

**반례**:
```
제목: "○○○ 후보 지지율 45%"
내용: ○○○ 후보의 지지율이 45%로 조사됐다.
```
→ **라벨: 1 (중립)** (단순 사실)

### 케이스 2: 긍정+부정 혼재

```
제목: "△△△ 후보, 경제 공약은 호평, 외교는 우려"
내용: △△△ 후보의 경제 공약은 전문가들로부터 호평받았으나,
외교 정책에 대해서는 우려의 목소리가 나왔다.
```

**판단 기준**:
- 전체 기사의 **비중** 확인
- 더 많이 강조된 쪽으로 판단
- 5:5 균형이면 → **라벨: 1 (중립)**

### 케이스 3: 인용문이 비판적이지만 기사는 중립

```
제목: "◇◇◇ 후보 정책, 야당 '포퓰리즘' 비난"
내용: ◇◇◇ 후보의 복지 정책에 대해 야당이 "비현실적
포퓰리즘"이라고 비판했다. ◇◇◇ 측은 "충분한 재원이
마련되어 있다"고 반박했다.
```

**판단**:
- 양측 의견 균형있게 제시
- 기자의 판단 없음
- → **라벨: 1 (중립)**

### 케이스 4: 중립적 표현이지만 전체 맥락은 비판

```
제목: "●●● 후보, '이번에도' 공약 발표"
내용: ●●● 후보가 오늘 또 다른 공약을 발표했다.
```

**판단**:
- "이번에도", "또 다른" → 빈번함을 암시 (부정적 뉘앙스)
- 맥락상 비판적 톤
- → **라벨: 2 (비판)**

### 애매할 때 원칙

#### 원칙 1: 보수적 판단
확실하지 않으면 **중립(1)**으로 판단

#### 원칙 2: 제목 vs 내용 불일치
**내용**을 우선시 (제목은 종종 자극적)

#### 원칙 3: 맥락 고려
정치적 맥락, 시기, 상황 고려

#### 원칙 4: 다수 의견
팀원들과 의견이 엇갈리면 다수결 또는 토론

---

## 팀 검수 방법

### A. 독립 검수 + 교차 검증 (권장)

#### 1단계: 분담
- 전체 1,909건을 팀원 수로 나눔
- 각자 독립적으로 검수

#### 2단계: 샘플 교차 검증
- 각자 검수한 것 중 **20% 무작위 샘플**을 다른 팀원이 재검수
- 일치율 측정 (목표: 80% 이상)

#### 3단계: 불일치 해결
- 일치하지 않는 케이스는 팀 회의에서 토론
- 합의를 통해 최종 라벨 결정

### B. 이중 검수 (높은 정확도 필요 시)

#### 방법
- 모든 기사를 **2명이 독립적으로** 검수
- 일치하면 확정, 불일치하면 3번째 검수자가 최종 판단

#### 장점
- 높은 정확도
- 일관성 확보

#### 단점
- 시간 2배 소요

### C. 우선순위별 차등 검수

#### 고우선순위 (74건): 이중 검수
- OpenAI↔Claude 정반대 판단
- 2명이 독립 검수

#### 중우선순위 (1,835건): 단일 검수
- 1명이 검수 후 20% 샘플 교차 검증

---

## 실전 검수 가이드

### Excel/Google Sheets 사용법

#### 1. 파일 열기
```
data/disagreements_for_review.csv
```

#### 2. 필터 설정
**우선순위 1 추출 (옹호↔비판)**:
- `label_openai` = 0 AND `label_claude` = 2
- `label_openai` = 2 AND `label_claude` = 0

**특정 패턴 추출**:
- 예: "옹호 → 중립" 보기
- `label_openai` = 0 AND `label_claude` = 1

#### 3. 검수 진행
각 행에서:
1. `title`, `summary` 읽기
2. `label_openai`, `label_claude` 확인
3. `final_label`에 0/1/2 입력
4. `reviewed`에 TRUE 입력
5. `reviewer`에 이름 입력
6. `review_notes`에 간단한 메모 (선택)

#### 4. 진행 상황 추적
- `reviewed = TRUE` 필터로 완료 건수 확인
- 정기적으로 파일 저장

### 효율적 검수 팁

#### 💡 Tip 1: 키보드 단축키 활용
- 엑셀에서 셀 이동: Tab, Enter
- 빠른 입력으로 효율 증대

#### 💡 Tip 2: 배치 처리
- 비슷한 패턴을 모아서 검수
- 예: "옹호 → 중립" 패턴을 한번에 검토

#### 💡 Tip 3: 휴식
- 50건당 10분 휴식
- 피로 누적 시 판단력 저하

#### 💡 Tip 4: 일관성 체크
- 첫 100건 검수 후 팀원과 비교
- 초반에 기준 통일

---

## 검수 품질 관리

### 일관성 측정

#### Inter-Annotator Agreement (IAA)
- 같은 데이터를 2명 이상이 검수
- 일치율 계산: (일치 건수 / 전체 건수) × 100
- **목표: 80% 이상**

#### 샘플 검증
```python
# 일치율 계산 예시
total = 100  # 검증한 샘플 수
agreed = 85  # 일치한 건수
iaa = (agreed / total) * 100
print(f"일치율: {iaa}%")  # 85%
```

### 품질 기준

| 일치율 | 품질 평가 | 조치 |
|--------|----------|------|
| 90% 이상 | 우수 | 계속 진행 |
| 80-89% | 양호 | 애매한 케이스 재논의 |
| 70-79% | 보통 | 가이드라인 재교육 |
| 70% 미만 | 미흡 | 가이드라인 재정비 필요 |

---

## FAQ

### Q1. OpenAI와 Claude 중 어느 것을 더 신뢰해야 하나요?

**A**: 둘 다 참고만 하고, **본인의 판단**이 가장 중요합니다.
- OpenAI: 더 다양하게 분류 (옹호 25%, 중립 49%, 비판 26%)
- Claude Haiku: 중립 성향 강함 (옹호 5%, 중립 76%, 비판 19%)

### Q2. 시간이 얼마나 걸리나요?

**A**: 건당 평균 30초~1분
- 명확한 케이스: 20-30초
- 애매한 케이스: 1-2분
- 전체 1,909건: 약 16-32시간 (팀원 분담 시 단축)

### Q3. 정치적 성향이 판단에 영향을 줄 수 있나요?

**A**: 최대한 객관적으로 판단하되, 다음을 유의하세요:
- **사실과 의견 구분**: 기사가 전달하는 "사실"에 집중
- **균형 잡기**: 본인의 정치 성향과 반대되는 입장도 공정하게 판단
- **교차 검증**: 팀원과의 검증으로 편향 최소화

### Q4. 기사가 너무 짧거나 정보가 부족하면?

**A**:
- 제목만으로 판단하지 말고, 주어진 정보 범위 내에서 판단
- 정말 판단 불가능하면 `review_notes`에 기록하고 **중립(1)** 처리

### Q5. 두 모델 모두 틀린 것 같으면?

**A**:
- 본인의 판단이 우선!
- 두 모델과 다른 라벨도 얼마든지 선택 가능
- `review_notes`에 근거를 명확히 작성

### Q6. 검수 중 의견이 엇갈리면?

**A**:
1. 각자 판단 근거 설명
2. 가이드라인 체크리스트로 재확인
3. 여전히 불일치하면 다수결 또는 팀 리더 판단
4. 추후 유사 케이스 처리 기준으로 기록

### Q7. 같은 인물/정당이 여러 기사에 나오면?

**A**:
- **각 기사를 독립적으로** 판단
- 같은 인물이라도 기사마다 논조가 다를 수 있음
- 기사 간 맥락을 억지로 연결하지 말 것

---

## 체크리스트 요약

### 검수 시작 전
- [ ] 가이드라인 숙지
- [ ] CSV 파일 백업
- [ ] 팀원과 샘플 검수 연습 (일치율 확인)

### 검수 중
- [ ] 제목 + 내용 모두 읽기
- [ ] AI 라벨 참고만, 독립적 판단
- [ ] 체크리스트 확인
- [ ] 판단 근거 메모

### 검수 후
- [ ] 정기적으로 파일 저장
- [ ] 샘플 교차 검증
- [ ] 팀 회의에서 애매한 케이스 논의

---

## 문의 및 피드백

검수 중 문의사항이나 가이드라인 개선 제안이 있으면 팀 채널에 공유해주세요.

**Good Luck! 💪**

---

**문서 버전**: 1.0
**최종 수정**: 2025-11-09
**작성자**: AI Assistant
