"""
KoBERT ê¸°ë°˜ ë‰´ìŠ¤ ìŠ¤íƒ ìŠ¤ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (ê°œì„  ë²„ì „ v2)

ê°œì„  ì‚¬í•­:
1. Class Weighted Loss
2. Focal Loss ì˜µì…˜
3. Early Stopping (ê°•í™”)
4. Weight Decay (L2 regularization)
5. Dropout ì¦ê°€ (0.3 â†’ 0.4)
6. Gradient Clipping

Usage:
    python scripts/train_kobert_improved.py --data_dir data --output_dir models/kobert_stance
"""

import argparse
import os
import json
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from transformers import BertModel, AutoTokenizer

import pandas as pd
import numpy as np
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from tqdm.auto import tqdm

# ëœë¤ ì‹œë“œ ê³ ì •
SEED = 42
torch.manual_seed(SEED)
np.random.seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)


class EarlyStopping:
    """
    Early Stopping: ê²€ì¦ ì†ì‹¤ì´ ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´ í•™ìŠµ ì¤‘ë‹¨
    """
    def __init__(self, patience=3, min_delta=0.001, verbose=True):
        """
        Args:
            patience: ê°œì„ ì´ ì—†ì–´ë„ ê¸°ë‹¤ë¦´ epoch ìˆ˜
            min_delta: ê°œì„ ìœ¼ë¡œ ê°„ì£¼í•  ìµœì†Œ ë³€í™”ëŸ‰
            verbose: ë¡œê·¸ ì¶œë ¥ ì—¬ë¶€
        """
        self.patience = patience
        self.min_delta = min_delta
        self.verbose = verbose

        self.counter = 0
        self.best_loss = None
        self.early_stop = False
        self.best_model_state = None

    def __call__(self, val_loss, model):
        """ê²€ì¦ ì†ì‹¤ì„ ì²´í¬í•˜ê³  early stop ì—¬ë¶€ ê²°ì •"""
        if self.best_loss is None:
            self.best_loss = val_loss
            self.save_checkpoint(val_loss, model)

        elif val_loss > self.best_loss - self.min_delta:
            self.counter += 1
            if self.verbose:
                print(f'âš ï¸  EarlyStopping counter: {self.counter}/{self.patience}')
                print(f'   í˜„ì¬ Val Loss: {val_loss:.4f}, ìµœê³  Val Loss: {self.best_loss:.4f}')

            if self.counter >= self.patience:
                self.early_stop = True
                if self.verbose:
                    print(f'\nğŸ›‘ Early Stopping! {self.patience} epoch ë™ì•ˆ ê°œì„  ì—†ìŒ')

        else:
            self.save_checkpoint(val_loss, model)
            self.counter = 0

    def save_checkpoint(self, val_loss, model):
        """ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥"""
        if self.verbose:
            if self.best_loss is not None:
                print(f'âœ… Val Loss ê°œì„ : {self.best_loss:.4f} â†’ {val_loss:.4f}')
            else:
                print(f'âœ… ìµœì´ˆ ëª¨ë¸ ì €ì¥ (Val Loss: {val_loss:.4f})')

        self.best_loss = val_loss
        # ëª¨ë¸ state dict ì €ì¥ (ë©”ëª¨ë¦¬ì—)
        self.best_model_state = {
            key: value.cpu().clone() for key, value in model.state_dict().items()
        }

    def load_best_model(self, model):
        """ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ"""
        if self.best_model_state is not None:
            model.load_state_dict(
                {key: value.to(next(model.parameters()).device)
                 for key, value in self.best_model_state.items()}
            )
            if self.verbose:
                print(f'âœ… ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (Val Loss: {self.best_loss:.4f})')
        return model


class FocalLoss(nn.Module):
    """
    Focal Loss - ì–´ë ¤ìš´ ìƒ˜í”Œì— ì§‘ì¤‘í•˜ë„ë¡ í•˜ëŠ” ì†ì‹¤ í•¨ìˆ˜
    í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¬¸ì œì— íš¨ê³¼ì 
    """
    def __init__(self, alpha=None, gamma=2.0):
        super(FocalLoss, self).__init__()
        self.alpha = alpha  # í´ë˜ìŠ¤ë³„ ê°€ì¤‘ì¹˜
        self.gamma = gamma  # focusing parameter

    def forward(self, inputs, targets):
        ce_loss = F.cross_entropy(inputs, targets, reduction='none', weight=self.alpha)
        pt = torch.exp(-ce_loss)
        focal_loss = ((1 - pt) ** self.gamma * ce_loss).mean()
        return focal_loss


class StanceDataset(Dataset):
    """ìŠ¤íƒ ìŠ¤ ë¶„ë¥˜ ë°ì´í„°ì…‹"""

    def __init__(self, texts, labels, tokenizer, max_length=512):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]

        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'label': torch.tensor(label, dtype=torch.long)
        }


class StanceClassifier(nn.Module):
    """KoBERT ê¸°ë°˜ ìŠ¤íƒ ìŠ¤ ë¶„ë¥˜ ëª¨ë¸"""

    def __init__(self, n_classes=3, dropout=0.4, model_name='skt/kobert-base-v1'):
        super(StanceClassifier, self).__init__()
        self.bert = BertModel.from_pretrained(model_name)
        self.dropout = nn.Dropout(dropout)
        self.classifier = nn.Linear(self.bert.config.hidden_size, n_classes)

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask
        )

        # [CLS] í† í°ì˜ ì¶œë ¥ ì‚¬ìš©
        pooled_output = outputs.pooler_output
        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)

        return logits


def calculate_class_weights(labels, device):
    """
    í´ë˜ìŠ¤ë³„ ê°€ì¤‘ì¹˜ ê³„ì‚°
    ì ì€ í´ë˜ìŠ¤ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜ ë¶€ì—¬
    """
    unique, counts = np.unique(labels, return_counts=True)
    total = len(labels)

    # ì—­ë¹ˆë„ ê°€ì¤‘ì¹˜ ê³„ì‚°
    weights = total / (len(unique) * counts)

    # ì •ê·œí™”
    weights = weights / weights.sum() * len(unique)

    class_weights = torch.FloatTensor(weights).to(device)

    print("\ní´ë˜ìŠ¤ë³„ ê°€ì¤‘ì¹˜:")
    label_names = ['ì˜¹í˜¸', 'ì¤‘ë¦½', 'ë¹„íŒ']
    for i, (count, weight) in enumerate(zip(counts, weights)):
        print(f"  {label_names[i]}: {count}ê°œ â†’ ê°€ì¤‘ì¹˜ {weight:.4f}")

    return class_weights


def train_epoch(model, data_loader, criterion, optimizer, device, epoch_num, max_grad_norm=1.0):
    """í•œ ì—í­ í•™ìŠµ"""
    model.train()
    total_loss = 0
    correct = 0
    total = 0

    # í´ë˜ìŠ¤ë³„ ì˜ˆì¸¡ ì¹´ìš´íŠ¸
    class_predictions = {0: 0, 1: 0, 2: 0}

    progress_bar = tqdm(data_loader, desc=f'Epoch {epoch_num} Training')

    for batch in progress_bar:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['label'].to(device)

        optimizer.zero_grad()

        outputs = model(input_ids, attention_mask)
        loss = criterion(outputs, labels)

        loss.backward()
        # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ (Exploding gradient ë°©ì§€)
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_grad_norm)
        optimizer.step()

        total_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

        # ì˜ˆì¸¡ í´ë˜ìŠ¤ ì¹´ìš´íŠ¸
        for pred in predicted.cpu().numpy():
            class_predictions[pred] += 1

        progress_bar.set_postfix({
            'loss': f'{loss.item():.4f}',
            'acc': f'{100 * correct / total:.2f}%'
        })

    avg_loss = total_loss / len(data_loader)
    accuracy = correct / total

    # í´ë˜ìŠ¤ë³„ ì˜ˆì¸¡ ë¶„í¬ ì¶œë ¥
    print(f"\n  ì˜ˆì¸¡ ë¶„í¬: ì˜¹í˜¸={class_predictions[0]}, ì¤‘ë¦½={class_predictions[1]}, ë¹„íŒ={class_predictions[2]}")

    return avg_loss, accuracy


def eval_model(model, data_loader, criterion, device, desc='Evaluating'):
    """ëª¨ë¸ í‰ê°€"""
    model.eval()
    total_loss = 0
    correct = 0
    total = 0

    with torch.no_grad():
        for batch in tqdm(data_loader, desc=desc):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['label'].to(device)

            outputs = model(input_ids, attention_mask)
            loss = criterion(outputs, labels)

            total_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    avg_loss = total_loss / len(data_loader)
    accuracy = correct / total

    return avg_loss, accuracy


def get_predictions(model, data_loader, device):
    """ì˜ˆì¸¡ê°’ ì–»ê¸°"""
    model.eval()
    predictions = []
    true_labels = []

    with torch.no_grad():
        for batch in tqdm(data_loader, desc='Getting predictions'):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['label'].to(device)

            outputs = model(input_ids, attention_mask)
            _, preds = torch.max(outputs, 1)

            predictions.extend(preds.cpu().numpy())
            true_labels.extend(labels.cpu().numpy())

    return np.array(predictions), np.array(true_labels)


def main(args):
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\n{'='*60}")
    print(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    print(f"{'='*60}\n")

    # ë°ì´í„° ë¡œë“œ
    print("ë°ì´í„° ë¡œë”© ì¤‘...")
    train_df = pd.read_csv(os.path.join(args.data_dir, 'train_dataset.csv'))
    val_df = pd.read_csv(os.path.join(args.data_dir, 'val_dataset.csv'))
    test_df = pd.read_csv(os.path.join(args.data_dir, 'test_dataset.csv'))

    print(f"\ní•™ìŠµ ë°ì´í„°: {len(train_df)}ê°œ")
    print(f"ê²€ì¦ ë°ì´í„°: {len(val_df)}ê°œ")
    print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_df)}ê°œ")

    # ë¼ë²¨ ë¶„í¬ ì¶œë ¥
    print(f"\ní•™ìŠµ ë°ì´í„° ë¼ë²¨ ë¶„í¬:")
    for label_id, count in train_df['label'].value_counts().sort_index().items():
        label_name = ['ì˜¹í˜¸', 'ì¤‘ë¦½', 'ë¹„íŒ'][label_id]
        print(f"  {label_id} ({label_name}): {count}ê°œ ({count/len(train_df)*100:.1f}%)")

    # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚°
    class_weights = calculate_class_weights(train_df['label'].values, device)

    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    print(f"\ní† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘... ({args.model_name})")
    try:
        tokenizer = AutoTokenizer.from_pretrained(args.model_name, trust_remote_code=True)
    except:
        print("ëŒ€ì•ˆ í† í¬ë‚˜ì´ì € ì‚¬ìš©: monologg/kobert")
        tokenizer = AutoTokenizer.from_pretrained('monologg/kobert')
    print(f"Vocab í¬ê¸°: {len(tokenizer)}")

    # Dataset ìƒì„±
    train_dataset = StanceDataset(
        train_df['text'].values,
        train_df['label'].values,
        tokenizer,
        args.max_length
    )
    val_dataset = StanceDataset(
        val_df['text'].values,
        val_df['label'].values,
        tokenizer,
        args.max_length
    )
    test_dataset = StanceDataset(
        test_df['text'].values,
        test_df['label'].values,
        tokenizer,
        args.max_length
    )

    # DataLoader ìƒì„±
    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size)
    test_loader = DataLoader(test_dataset, batch_size=args.batch_size)

    # ëª¨ë¸ ì´ˆê¸°í™”
    print(f"\nëª¨ë¸ ë¡œë”© ì¤‘...")
    model = StanceClassifier(
        n_classes=3,
        dropout=args.dropout,
        model_name=args.model_name
    )
    model = model.to(device)

    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"ì „ì²´ íŒŒë¼ë¯¸í„°: {total_params:,}")
    print(f"í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°: {trainable_params:,}")

    # ì†ì‹¤ í•¨ìˆ˜ ì„ íƒ
    if args.use_focal_loss:
        print("\nì†ì‹¤ í•¨ìˆ˜: Focal Loss (gamma={})".format(args.focal_gamma))
        criterion = FocalLoss(alpha=class_weights, gamma=args.focal_gamma)
    else:
        print("\nì†ì‹¤ í•¨ìˆ˜: Weighted Cross Entropy")
        criterion = nn.CrossEntropyLoss(weight=class_weights)

    # ì˜µí‹°ë§ˆì´ì € (Weight Decay ëª…ì‹œì  ì„¤ì •!)
    optimizer = optim.AdamW(
        model.parameters(),
        lr=args.learning_rate,
        weight_decay=args.weight_decay,  # L2 regularization
        eps=1e-8
    )

    # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ (validation loss ê¸°ì¤€ìœ¼ë¡œ ë³€ê²½)
    from torch.optim.lr_scheduler import ReduceLROnPlateau
    scheduler = ReduceLROnPlateau(
        optimizer,
        mode='min',        # validation lossë¥¼ ìµœì†Œí™”
        factor=0.5,        # LRì„ ì ˆë°˜ìœ¼ë¡œ
        patience=2,        # 2 epoch ë™ì•ˆ ê°œì„  ì—†ìœ¼ë©´ LR ê°ì†Œ
        verbose=True,
        min_lr=1e-7        # ìµœì†Œ learning rate
    )

    # Early Stopping ì´ˆê¸°í™”
    early_stopping = EarlyStopping(
        patience=args.early_stop_patience,
        min_delta=0.001,
        verbose=True
    )

    # í•™ìŠµ ê¸°ë¡
    history = {
        'train_loss': [],
        'train_acc': [],
        'val_loss': [],
        'val_acc': [],
        'learning_rates': []  # LR ë³€í™” ì¶”ì 
    }

    # í•™ìŠµ ë£¨í”„
    print(f"\n{'='*60}")
    print("í•™ìŠµ ì‹œì‘ (Early Stopping í™œì„±í™”)")
    print(f"{'='*60}")
    print(f"ìµœëŒ€ Epoch: {args.epochs}")
    print(f"Early Stop Patience: {args.early_stop_patience}")
    print(f"Batch Size: {args.batch_size}")
    print(f"Learning Rate: {args.learning_rate}")
    print(f"Weight Decay: {args.weight_decay}")
    print(f"Dropout: {args.dropout}")
    print(f"Max Grad Norm: {args.max_grad_norm}")
    print("="*60 + "\n")

    best_val_f1 = 0
    best_epoch = 0

    for epoch in range(args.epochs):
        print(f"\n{'='*60}")
        print(f"Epoch {epoch + 1}/{args.epochs}")
        print(f"{'='*60}")

        # í˜„ì¬ Learning Rate ì¶œë ¥
        current_lr = optimizer.param_groups[0]['lr']
        print(f"ğŸ“Š Current Learning Rate: {current_lr:.2e}")

        # í•™ìŠµ
        train_loss, train_acc = train_epoch(
            model, train_loader, criterion, optimizer, device, epoch + 1,
            max_grad_norm=args.max_grad_norm
        )
        print(f"\nğŸ“Š Train Loss: {train_loss:.4f}, Train Acc: {train_acc*100:.2f}%")

        # ê²€ì¦
        val_loss, val_acc = eval_model(model, val_loader, criterion, device, 'Validating')

        # ê²€ì¦ F1 ê³„ì‚°
        val_preds, val_true = get_predictions(model, val_loader, device)
        from sklearn.metrics import f1_score
        val_f1_macro = f1_score(val_true, val_preds, average='macro')

        print(f"ğŸ“Š Val Loss: {val_loss:.4f}, Val Acc: {val_acc*100:.2f}%, Val F1 (macro): {val_f1_macro:.4f}")

        # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸ (Val Loss ê¸°ì¤€ìœ¼ë¡œ ë³€ê²½)
        scheduler.step(val_loss)

        # ê¸°ë¡ ì €ì¥
        history['train_loss'].append(train_loss)
        history['train_acc'].append(train_acc)
        history['val_loss'].append(val_loss)
        history['val_acc'].append(val_acc)
        history['learning_rates'].append(current_lr)

        # ìµœê³  F1 ì„±ëŠ¥ ì¶”ì 
        if val_f1_macro > best_val_f1:
            best_val_f1 = val_f1_macro
            best_epoch = epoch + 1
            print(f"âœ¨ ìƒˆë¡œìš´ ìµœê³  ê²€ì¦ F1: {best_val_f1:.4f}")

        # Early Stopping ì²´í¬ (Val Loss ê¸°ì¤€)
        early_stopping(val_loss, model)

        if early_stopping.early_stop:
            print(f"\n{'='*60}")
            print(f"ğŸ›‘ Early Stopping at Epoch {epoch + 1}")
            print(f"{'='*60}")
            break

    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ
    print(f"\n{'='*60}")
    print("ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ")
    print(f"{'='*60}")
    model = early_stopping.load_best_model(model)
    print(f"âœ… ìµœê³  ê²€ì¦ F1: {best_val_f1:.4f} (Epoch {best_epoch})")

    print(f"\n{'='*60}")
    print("í•™ìŠµ ì™„ë£Œ!")
    print(f"{'='*60}")
    print(f"ì´ í•™ìŠµ Epoch: {epoch + 1}/{args.epochs}")
    print(f"ìµœê³  ê²€ì¦ F1: {best_val_f1:.4f} (Epoch {best_epoch})")
    print(f"ìµœì¢… Val Loss: {early_stopping.best_loss:.4f}")

    # í…ŒìŠ¤íŠ¸ í‰ê°€
    print(f"\n{'='*60}")
    print("í…ŒìŠ¤íŠ¸ í‰ê°€")
    print(f"{'='*60}\n")

    test_loss, test_acc = eval_model(model, test_loader, criterion, device, 'Testing')
    print(f"\nTest Loss: {test_loss:.4f}")
    print(f"Test Accuracy: {test_acc*100:.2f}%")

    # ìƒì„¸ ë¶„ë¥˜ ë³´ê³ ì„œ
    predictions, true_labels = get_predictions(model, test_loader, device)
    target_names = ['ì˜¹í˜¸ (Support)', 'ì¤‘ë¦½ (Neutral)', 'ë¹„íŒ (Oppose)']
    print("\në¶„ë¥˜ ë³´ê³ ì„œ:")
    print(classification_report(
        true_labels, predictions,
        target_names=target_names,
        digits=4,
        zero_division=0
    ))

    # Confusion Matrix
    cm = confusion_matrix(true_labels, predictions)
    print("\nConfusion Matrix:")
    print("              Predicted")
    print("              Support  Neutral  Oppose")
    print(f"Actual Support   {cm[0][0]:3d}      {cm[0][1]:3d}      {cm[0][2]:3d}")
    print(f"       Neutral   {cm[1][0]:3d}      {cm[1][1]:3d}      {cm[1][2]:3d}")
    print(f"       Oppose    {cm[2][0]:3d}      {cm[2][1]:3d}      {cm[2][2]:3d}")

    # ëª¨ë¸ ì €ì¥
    os.makedirs(args.output_dir, exist_ok=True)

    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    model_filename = f'kobert_stance_improved_{timestamp}.pth'
    model_path = os.path.join(args.output_dir, model_filename)

    torch.save(model.state_dict(), model_path)
    print(f"\nëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_path}")

    # ë©”íƒ€ë°ì´í„° ì €ì¥
    metadata = {
        'timestamp': timestamp,
        'test_accuracy': float(test_acc),
        'test_loss': float(test_loss),
        'best_val_f1_macro': float(best_val_f1),
        'hyperparameters': {
            'max_length': args.max_length,
            'batch_size': args.batch_size,
            'learning_rate': args.learning_rate,
            'epochs': args.epochs,
            'dropout': args.dropout,
            'use_focal_loss': args.use_focal_loss,
            'focal_gamma': args.focal_gamma if args.use_focal_loss else None
        },
        'data_info': {
            'train_samples': len(train_df),
            'val_samples': len(val_df),
            'test_samples': len(test_df)
        },
        'model_name': args.model_name,
        'class_weights': class_weights.cpu().tolist(),
        'confusion_matrix': cm.tolist(),
        'classification_report': classification_report(
            true_labels, predictions,
            target_names=target_names,
            digits=4,
            zero_division=0,
            output_dict=True
        )
    }

    metadata_filename = f'metadata_improved_{timestamp}.json'
    metadata_path = os.path.join(args.output_dir, metadata_filename)

    with open(metadata_path, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)

    print(f"ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ: {metadata_path}")

    # í•™ìŠµ íˆìŠ¤í† ë¦¬ ì €ì¥
    history_filename = f'history_improved_{timestamp}.json'
    history_path = os.path.join(args.output_dir, history_filename)

    with open(history_path, 'w', encoding='utf-8') as f:
        json.dump(history, f, ensure_ascii=False, indent=2)

    print(f"í•™ìŠµ íˆìŠ¤í† ë¦¬ ì €ì¥ ì™„ë£Œ: {history_path}")

    print(f"\n{'='*60}")
    print("í•™ìŠµ ì™„ë£Œ!")
    print(f"{'='*60}\n")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='KoBERT ìŠ¤íƒ ìŠ¤ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ (ê°œì„  ë²„ì „)')

    # ë°ì´í„° ê´€ë ¨
    parser.add_argument('--data_dir', type=str, default='data',
                        help='ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ')
    parser.add_argument('--output_dir', type=str, default='models/kobert_stance',
                        help='ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬ ê²½ë¡œ')

    # ëª¨ë¸ ê´€ë ¨
    parser.add_argument('--model_name', type=str, default='skt/kobert-base-v1',
                        help='ì‚¬ìš©í•  KoBERT ëª¨ë¸ëª…')
    parser.add_argument('--max_length', type=int, default=512,
                        help='ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´')
    parser.add_argument('--dropout', type=float, default=0.4,
                        help='ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨ (ê³¼ì í•© ë°©ì§€ ê°•í™”: 0.4)')

    # í•™ìŠµ ê´€ë ¨
    parser.add_argument('--batch_size', type=int, default=16,
                        help='ë°°ì¹˜ í¬ê¸°')
    parser.add_argument('--learning_rate', type=float, default=2e-5,
                        help='í•™ìŠµë¥ ')
    parser.add_argument('--epochs', type=int, default=15,
                        help='ì—í­ ìˆ˜ (ìµœëŒ€)')
    parser.add_argument('--weight_decay', type=float, default=0.01,
                        help='Weight decay (L2 regularization)')
    parser.add_argument('--max_grad_norm', type=float, default=1.0,
                        help='Gradient clipping max norm')

    # Early Stopping ê´€ë ¨
    parser.add_argument('--early_stop_patience', type=int, default=3,
                        help='Early stopping patience (ê°œì„  ì—†ì„ ë•Œ ê¸°ë‹¤ë¦´ epoch ìˆ˜)')
    parser.add_argument('--patience', type=int, default=5,
                        help='(Deprecated) LR scheduler patience')

    # í´ë˜ìŠ¤ ë¶ˆê· í˜• ëŒ€ì‘
    parser.add_argument('--use_focal_loss', action='store_true',
                        help='Focal Loss ì‚¬ìš© (ê¸°ë³¸ê°’: Weighted CE)')
    parser.add_argument('--focal_gamma', type=float, default=2.0,
                        help='Focal Loss gamma ê°’')

    args = parser.parse_args()

    main(args)
