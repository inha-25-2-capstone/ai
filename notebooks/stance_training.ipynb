{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KoBERT ê¸°ë°˜ ë‰´ìŠ¤ ìŠ¤íƒ ìŠ¤ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ **ì˜¹í˜¸(Support)**, **ì¤‘ë¦½(Neutral)**, **ë¹„íŒ(Oppose)** 3ê°œ í´ë˜ìŠ¤ë¡œ ë¶„ë¥˜í•˜ëŠ” KoBERT ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.\n",
    "\n",
    "## ëª©ì°¨\n",
    "1. [í™˜ê²½ ì„¤ì •](#1-í™˜ê²½-ì„¤ì •)\n",
    "2. [ë°ì´í„° ì¤€ë¹„](#2-ë°ì´í„°-ì¤€ë¹„)\n",
    "3. [ë°ì´í„° ì „ì²˜ë¦¬](#3-ë°ì´í„°-ì „ì²˜ë¦¬)\n",
    "4. [ëª¨ë¸ ì •ì˜](#4-ëª¨ë¸-ì •ì˜)\n",
    "5. [í•™ìŠµ](#5-í•™ìŠµ)\n",
    "6. [í‰ê°€](#6-í‰ê°€)\n",
    "7. [ëª¨ë¸ ì €ì¥](#7-ëª¨ë¸-ì €ì¥)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì„¤ì •\n",
    "\n",
    "í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•˜ê³  ì„í¬íŠ¸í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab í™˜ê²½ í™•ì¸\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"âœ… Google Colab í™˜ê²½ì—ì„œ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.\")\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"âš ï¸  ë¡œì»¬ í™˜ê²½ì—ì„œ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "!pip install -q torch transformers\n",
    "!pip install -q tokenizers\n",
    "!pip install -q scikit-learn pandas numpy matplotlib seaborn\n",
    "!pip install -q tqdm\n",
    "\n",
    "print(\"âœ… íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertModel, AutoTokenizer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ ì„¤ì • (matplotlib)\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# ëœë¤ ì‹œë“œ ê³ ì •\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# ë””ë°”ì´ìŠ¤ ì„¤ì •\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nğŸ–¥ï¸  ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ë°ì´í„° ì¤€ë¹„\n",
    "\n",
    "### ì˜µì…˜ 1: ìƒ˜í”Œ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸\n",
    "ì‹¤ì œ ë°ì´í„°ê°€ ì—†ì„ ë•Œ ìƒ˜í”Œ ë°ì´í„°ë¡œ ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒ˜í”Œ ë°ì´í„° ìƒì„± (í•™ìŠµì„ ìœ„í•´ ë” ë§ì€ ìƒ˜í”Œ ì¶”ê°€)\n",
    "sample_data = []\n",
    "\n",
    "# ì˜¹í˜¸ (Support) - ë ˆì´ë¸” 0\n",
    "support_samples = [\n",
    "    \"ì •ë¶€ì˜ ìƒˆë¡œìš´ ê²½ì œ ì •ì±…ì€ ê²½ì œ ì„±ì¥ì„ ì´‰ì§„í•˜ê³  ì¼ìë¦¬ë¥¼ ì°½ì¶œí•  ê²ƒìœ¼ë¡œ ê¸°ëŒ€ëœë‹¤. ì „ë¬¸ê°€ë“¤ì€ ì´ë²ˆ ì •ì±…ì´ ì‹œì˜ì ì ˆí•œ ì¡°ì¹˜ë¼ê³  í‰ê°€í–ˆë‹¤.\",\n",
    "    \"ì´ë²ˆ ê°œí˜ì•ˆì€ êµ­ë¯¼ì˜ ì‚¶ì˜ ì§ˆì„ í–¥ìƒì‹œí‚¤ëŠ” ë° í¬ê²Œ ê¸°ì—¬í•  ê²ƒì´ë‹¤. ì—¬ëŸ¬ ê²½ì œ ì§€í‘œë“¤ì´ ê¸ì •ì ì¸ ë³€í™”ë¥¼ ë³´ì—¬ì£¼ê³  ìˆë‹¤.\",\n",
    "    \"ì •ì±… ì‹œí–‰ í›„ ê´€ë ¨ ì—…ê³„ì˜ ë§¤ì¶œì´ ì¦ê°€í•˜ê³  ìˆìœ¼ë©°, ê³ ìš©ë¥ ë„ ìƒìŠ¹ì„¸ë¥¼ ë³´ì´ê³  ìˆë‹¤. ì •ì±…ì˜ íš¨ê³¼ê°€ ê°€ì‹œí™”ë˜ê³  ìˆëŠ” ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤.\",\n",
    "    \"ì „ë¬¸ê°€ë“¤ì€ ì´ë²ˆ ì¡°ì¹˜ê°€ ì¥ê¸°ì ìœ¼ë¡œ êµ­ê°€ ê²½ìŸë ¥ì„ ë†’ì´ëŠ” ë° ë„ì›€ì´ ë  ê²ƒì´ë¼ê³  ë¶„ì„í–ˆë‹¤. í•´ì™¸ ì‚¬ë¡€ì—ì„œë„ ìœ ì‚¬í•œ ì„±ê³¼ë¥¼ ê±°ë‘” ë°” ìˆë‹¤.\",\n",
    "    \"ì‹œë¯¼ë‹¨ì²´ë“¤ë„ ì´ë²ˆ ì •ì±…ì— ëŒ€í•´ ê¸ì •ì ì¸ ë°˜ì‘ì„ ë³´ì´ê³  ìˆìœ¼ë©°, êµ­ë¯¼ ë§Œì¡±ë„ ì¡°ì‚¬ì—ì„œë„ ë†’ì€ ì ìˆ˜ë¥¼ ë°›ì•˜ë‹¤.\",\n",
    "    \"ì´ë²ˆ ë²•ì•ˆì€ êµ­ë¯¼ì˜ ê¶Œë¦¬ë¥¼ ë³´í˜¸í•˜ê³  ì‚¬íšŒ ì•ˆì „ë§ì„ ê°•í™”í•˜ëŠ” ë° í° ë„ì›€ì´ ë  ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤.\",\n",
    "    \"ì •ì±… íš¨ê³¼ê°€ ì‹¤ì œ í†µê³„ë¡œ ì…ì¦ë˜ê³  ìˆìœ¼ë©°, ê´€ë ¨ ë¶„ì•¼ ì „ë¬¸ê°€ë“¤ë„ ê¸ì •ì ì¸ í‰ê°€ë¥¼ ë‚´ë¦¬ê³  ìˆë‹¤.\",\n",
    "    \"ì´ë²ˆ ì¡°ì¹˜ëŠ” êµ­ì œ ì‚¬íšŒì—ì„œë„ ëª¨ë²” ì‚¬ë¡€ë¡œ í‰ê°€ë°›ê³  ìˆìœ¼ë©°, ì—¬ëŸ¬ êµ­ê°€ê°€ ë²¤ì¹˜ë§ˆí‚¹ì„ ì›í•˜ê³  ìˆë‹¤.\",\n",
    "]\n",
    "\n",
    "# ì¤‘ë¦½ (Neutral) - ë ˆì´ë¸” 1\n",
    "neutral_samples = [\n",
    "    \"ì •ë¶€ê°€ ìƒˆë¡œìš´ ì •ì±…ì„ ë°œí‘œí–ˆë‹¤. ì •ì±…ì˜ ì£¼ìš” ë‚´ìš©ì€ ê²½ì œ í™œì„±í™”ì™€ ì¼ìë¦¬ ì°½ì¶œì´ë‹¤. ì‹œí–‰ ì‹œê¸°ëŠ” ë‚´ë…„ ì´ˆë¡œ ì˜ˆì •ë˜ì–´ ìˆë‹¤.\",\n",
    "    \"ì´ë²ˆ ê°œí˜ì•ˆì— ëŒ€í•´ ì „ë¬¸ê°€ë“¤ì˜ ì˜ê²¬ì´ ì—‡ê°ˆë¦¬ê³  ìˆë‹¤. ì¼ë¶€ëŠ” ê¸ì •ì ìœ¼ë¡œ í‰ê°€í•˜ëŠ” ë°˜ë©´, ë‹¤ë¥¸ ì „ë¬¸ê°€ë“¤ì€ ì‹ ì¤‘í•œ ì ‘ê·¼ì´ í•„ìš”í•˜ë‹¤ê³  ë§í–ˆë‹¤.\",\n",
    "    \"ì •ì±… ë°œí‘œ í›„ ê´€ë ¨ ì—…ê³„ì—ì„œëŠ” ë‹¤ì–‘í•œ ë°˜ì‘ì´ ë‚˜ì˜¤ê³  ìˆë‹¤. êµ¬ì²´ì ì¸ ì‹œí–‰ ë°©ì•ˆì— ëŒ€í•œ ë…¼ì˜ê°€ ì§„í–‰ ì¤‘ì´ë‹¤.\",\n",
    "    \"ìƒˆë¡œìš´ ì œë„ê°€ ë„ì…ë˜ë©´ ê¸°ì¡´ ì‹œìŠ¤í…œì— ë³€í™”ê°€ ìƒê¸¸ ê²ƒìœ¼ë¡œ ì˜ˆìƒëœë‹¤. ì •ë¶€ëŠ” ê´€ê³„ ë¶€ì²˜ í˜‘ì˜ë¥¼ í†µí•´ ì„¸ë¶€ ì‚¬í•­ì„ ì¡°ìœ¨í•˜ê³  ìˆë‹¤.\",\n",
    "    \"ì´ë²ˆ ì¡°ì¹˜ì— ëŒ€í•œ êµ­ë¯¼ ì—¬ë¡ ì€ ì•„ì§ í™•ì •ë˜ì§€ ì•Šì•˜ë‹¤. í–¥í›„ ì¶”ì´ë¥¼ ì§€ì¼œë´ì•¼ í•  ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤.\",\n",
    "    \"ì •ë¶€ê°€ ì˜¤ëŠ˜ ìƒˆë¡œìš´ ê²½ì œ ì •ì±…ì„ ê³µì‹ ë°œí‘œí–ˆë‹¤. ì£¼ìš” ë‚´ìš©ì€ ë‹¤ìŒ ì£¼ ìƒì„¸íˆ ê³µê°œë  ì˜ˆì •ì´ë‹¤.\",\n",
    "    \"ê´€ë ¨ ë²•ì•ˆì´ êµ­íšŒì— ì œì¶œë˜ì—ˆìœ¼ë©°, í˜„ì¬ ìƒì„ìœ„ì›íšŒì—ì„œ ê²€í†  ì¤‘ì´ë‹¤. í†µê³¼ ì‹œê¸°ëŠ” ë¯¸ì •ì´ë‹¤.\",\n",
    "    \"ì „ë¬¸ê°€ë“¤ì€ ì´ë²ˆ ì •ì±…ì˜ íš¨ê³¼ë¥¼ íŒë‹¨í•˜ê¸° ìœ„í•´ì„œëŠ” ìµœì†Œ 6ê°œì›” ì´ìƒì˜ ì‹œê°„ì´ í•„ìš”í•˜ë‹¤ê³  ë§í–ˆë‹¤.\",\n",
    "]\n",
    "\n",
    "# ë¹„íŒ (Oppose) - ë ˆì´ë¸” 2\n",
    "oppose_samples = [\n",
    "    \"ì •ë¶€ì˜ ì •ì±…ì€ í˜„ì‹¤ì„ ì œëŒ€ë¡œ ë°˜ì˜í•˜ì§€ ëª»í•œ ì±„ ì¡¸ì†ìœ¼ë¡œ ì¶”ì§„ë˜ê³  ìˆë‹¤. ì „ë¬¸ê°€ë“¤ì€ ë¶€ì‘ìš©ì„ ìš°ë ¤í•˜ê³  ìˆë‹¤.\",\n",
    "    \"ì´ë²ˆ ê°œí˜ì•ˆì€ ì‹¤íš¨ì„±ì´ ì˜ì‹¬ëœë‹¤. ê³¼ê±° ìœ ì‚¬í•œ ì •ì±…ë“¤ì´ ì‹¤íŒ¨í–ˆë˜ ì „ë¡€ë¥¼ ê³ ë ¤í•  ë•Œ, ì‹ ì¤‘í•œ ì¬ê²€í† ê°€ í•„ìš”í•˜ë‹¤.\",\n",
    "    \"ì •ì±… ì‹œí–‰ìœ¼ë¡œ ì¸í•´ ì¤‘ì†Œê¸°ì—…ë“¤ì˜ ë¶€ë‹´ì´ ê°€ì¤‘ë  ê²ƒì´ë¼ëŠ” ìš°ë ¤ê°€ ì œê¸°ë˜ê³  ìˆë‹¤. ì—…ê³„ëŠ” ê°•ë ¥íˆ ë°˜ë°œí•˜ê³  ìˆë‹¤.\",\n",
    "    \"ì—¬ëŸ¬ ë¬¸ì œì ì´ ì§€ì ë˜ê³  ìˆìŒì—ë„ ë¶ˆêµ¬í•˜ê³  ì •ë¶€ëŠ” ì¼ë°©ì ìœ¼ë¡œ ì¶”ì§„í•˜ê³  ìˆë‹¤. êµ­ë¯¼ì˜ ëª©ì†Œë¦¬ë¥¼ ë“£ì§€ ì•ŠëŠ”ë‹¤ëŠ” ë¹„íŒì´ ë‚˜ì˜¨ë‹¤.\",\n",
    "    \"ì „ë¬¸ê°€ë“¤ì€ ì´ë²ˆ ì¡°ì¹˜ê°€ ì˜¤íˆë ¤ ê²½ì œì— ì•…ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆë‹¤ê³  ê²½ê³ í–ˆë‹¤. ì‹œì¥ì˜ ë°˜ì‘ë„ ë¶€ì •ì ì´ë‹¤.\",\n",
    "    \"ì´ë²ˆ ì •ì±…ì€ ì¤€ë¹„ ë¶€ì¡±ê³¼ ì¡¸ì† ì²˜ë¦¬ë¡œ ì¸í•´ í˜¼ë€ë§Œ ê°€ì¤‘ì‹œí‚¤ê³  ìˆë‹¤ëŠ” ë¹„íŒì´ ê±°ì„¸ë‹¤.\",\n",
    "    \"ì•¼ë‹¹ì€ ì´ë²ˆ ë²•ì•ˆì´ í—Œë²• ì •ì‹ ì— ìœ„ë°°ëœë‹¤ë©° ê°•ë ¥íˆ ë°˜ëŒ€í•˜ê³  ìˆìœ¼ë©°, ì² íšŒë¥¼ ìš”êµ¬í•˜ê³  ìˆë‹¤.\",\n",
    "    \"ì‹œë¯¼ë‹¨ì²´ë“¤ì€ ì´ë²ˆ ì¡°ì¹˜ê°€ êµ­ë¯¼ì˜ ê¶Œë¦¬ë¥¼ ì¹¨í•´í•  ìˆ˜ ìˆë‹¤ë©° ìš°ë ¤ë¥¼ í‘œëª…í•˜ê³  ìˆë‹¤.\",\n",
    "]\n",
    "\n",
    "# ë°ì´í„° ìƒì„±\n",
    "for text in support_samples:\n",
    "    sample_data.append({\"text\": text, \"label\": 0})\n",
    "for text in neutral_samples:\n",
    "    sample_data.append({\"text\": text, \"label\": 1})\n",
    "for text in oppose_samples:\n",
    "    sample_data.append({\"text\": text, \"label\": 2})\n",
    "\n",
    "# DataFrame ìƒì„±\n",
    "sample_df = pd.DataFrame(sample_data)\n",
    "print(f\"\\nğŸ“Š ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì™„ë£Œ: {len(sample_df)}ê°œ\")\n",
    "print(f\"\\ní´ë˜ìŠ¤ë³„ ë¶„í¬:\")\n",
    "print(sample_df['label'].value_counts().sort_index())\n",
    "print(f\"\\n0: ì˜¹í˜¸ (Support)\")\n",
    "print(f\"1: ì¤‘ë¦½ (Neutral)\")\n",
    "print(f\"2: ë¹„íŒ (Oppose)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì˜µì…˜ 2: ì‹¤ì œ ë°ì´í„° ì—…ë¡œë“œ\n",
    "\n",
    "ì‹¤ì œ ë¼ë²¨ë§ëœ ë°ì´í„°ê°€ ìˆë‹¤ë©´ ì•„ë˜ ì…€ì„ ì‹¤í–‰í•˜ì—¬ ì—…ë¡œë“œí•˜ì„¸ìš”.\n",
    "\n",
    "**ë°ì´í„° í˜•ì‹ (CSV):**\n",
    "```csv\n",
    "text,label\n",
    "\"ê¸°ì‚¬ ë³¸ë¬¸ 1\",0\n",
    "\"ê¸°ì‚¬ ë³¸ë¬¸ 2\",1\n",
    "\"ê¸°ì‚¬ ë³¸ë¬¸ 3\",2\n",
    "```\n",
    "\n",
    "**ë°ì´í„° í˜•ì‹ (JSON):**\n",
    "```json\n",
    "[\n",
    "  {\"text\": \"ê¸°ì‚¬ ë³¸ë¬¸ 1\", \"label\": 0},\n",
    "  {\"text\": \"ê¸°ì‚¬ ë³¸ë¬¸ 2\", \"label\": 1},\n",
    "  {\"text\": \"ê¸°ì‚¬ ë³¸ë¬¸ 3\", \"label\": 2}\n",
    "]\n",
    "```\n",
    "\n",
    "**ë ˆì´ë¸”:**\n",
    "- 0: ì˜¹í˜¸ (Support)\n",
    "- 1: ì¤‘ë¦½ (Neutral)\n",
    "- 2: ë¹„íŒ (Oppose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹¤ì œ ë°ì´í„° ì—…ë¡œë“œ (ì„ íƒì‚¬í•­)\n",
    "USE_REAL_DATA = False  # Trueë¡œ ë³€ê²½í•˜ì—¬ ì‹¤ì œ ë°ì´í„° ì‚¬ìš©\n",
    "\n",
    "if USE_REAL_DATA:\n",
    "    if IN_COLAB:\n",
    "        from google.colab import files\n",
    "        print(\"ğŸ“ ë°ì´í„° íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš” (CSV ë˜ëŠ” JSON)\")\n",
    "        uploaded = files.upload()\n",
    "        \n",
    "        # íŒŒì¼ëª… ê°€ì ¸ì˜¤ê¸°\n",
    "        filename = list(uploaded.keys())[0]\n",
    "        \n",
    "        # íŒŒì¼ í˜•ì‹ì— ë”°ë¼ ë¡œë“œ\n",
    "        if filename.endswith('.csv'):\n",
    "            df = pd.read_csv(filename)\n",
    "        elif filename.endswith('.json'):\n",
    "            df = pd.read_json(filename)\n",
    "        else:\n",
    "            raise ValueError(\"CSV ë˜ëŠ” JSON íŒŒì¼ë§Œ ì§€ì›ë©ë‹ˆë‹¤.\")\n",
    "    else:\n",
    "        # ë¡œì»¬ í™˜ê²½\n",
    "        data_path = input(\"ë°ì´í„° íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”: \")\n",
    "        if data_path.endswith('.csv'):\n",
    "            df = pd.read_csv(data_path)\n",
    "        elif data_path.endswith('.json'):\n",
    "            df = pd.read_json(data_path)\n",
    "        else:\n",
    "            raise ValueError(\"CSV ë˜ëŠ” JSON íŒŒì¼ë§Œ ì§€ì›ë©ë‹ˆë‹¤.\")\n",
    "    \n",
    "    print(f\"\\nâœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df)}ê°œ\")\n",
    "    print(f\"\\ní´ë˜ìŠ¤ë³„ ë¶„í¬:\")\n",
    "    print(df['label'].value_counts().sort_index())\n",
    "else:\n",
    "    # ìƒ˜í”Œ ë°ì´í„° ì‚¬ìš©\n",
    "    df = sample_df\n",
    "    print(\"\\nğŸ“ ìƒ˜í”Œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° í™•ì¸\n",
    "print(\"\\n=== ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° ===\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\n=== ë°ì´í„° ì •ë³´ ===\")\n",
    "print(f\"ì „ì²´ ìƒ˜í”Œ ìˆ˜: {len(df)}\")\n",
    "print(f\"\\ní´ë˜ìŠ¤ë³„ ë¶„í¬:\")\n",
    "label_counts = df['label'].value_counts().sort_index()\n",
    "for label, count in label_counts.items():\n",
    "    label_name = ['ì˜¹í˜¸', 'ì¤‘ë¦½', 'ë¹„íŒ'][label]\n",
    "    print(f\"  {label} ({label_name}): {count}ê°œ ({count/len(df)*100:.1f}%)\")\n",
    "\n",
    "# í´ë˜ìŠ¤ ë¶„í¬ ì‹œê°í™”\n",
    "plt.figure(figsize=(8, 5))\n",
    "label_names = ['ì˜¹í˜¸\\n(Support)', 'ì¤‘ë¦½\\n(Neutral)', 'ë¹„íŒ\\n(Oppose)']\n",
    "plt.bar(range(3), [label_counts.get(i, 0) for i in range(3)])\n",
    "plt.xticks(range(3), label_names)\n",
    "plt.ylabel('Count')\n",
    "plt.title('Class Distribution')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ë°ì´í„° ì „ì²˜ë¦¬\n",
    "\n",
    "KoBERT í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KoBERT í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "print(\"ğŸ”„ KoBERT í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘...\")\n",
    "try:\n",
    "    # AutoTokenizerë¥¼ ì‚¬ìš©í•˜ì—¬ KoBERT í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        'skt/kobert-base-v1',\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    print(\"âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ!\")\n",
    "    print(f\"   Vocab í¬ê¸°: {len(tokenizer)}\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "    print(\"\\nëŒ€ì•ˆ: monologg/kobert í† í¬ë‚˜ì´ì € ì‚¬ìš©\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained('monologg/kobert')\n",
    "    print(\"âœ… ëŒ€ì•ˆ í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ!\")\n",
    "    print(f\"   Vocab í¬ê¸°: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸\n",
    "test_text = \"ì´ê²ƒì€ í…ŒìŠ¤íŠ¸ ë¬¸ì¥ì…ë‹ˆë‹¤.\"\n",
    "encoded = tokenizer.encode_plus(\n",
    "    test_text,\n",
    "    add_special_tokens=True,\n",
    "    max_length=50,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "print(\"\\n=== í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸ ===\")\n",
    "print(f\"ì›ë³¸ í…ìŠ¤íŠ¸: {test_text}\")\n",
    "print(f\"í† í°í™” ê²°ê³¼ shape: {encoded['input_ids'].shape}\")\n",
    "print(f\"Input IDs (ì²˜ìŒ 20ê°œ): {encoded['input_ids'][0][:20].tolist()}\")\n",
    "print(\"âœ… í† í¬ë‚˜ì´ì € ì •ìƒ ì‘ë™!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì»¤ìŠ¤í…€ Dataset í´ë˜ìŠ¤\n",
    "class StanceDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "print(\"âœ… Dataset í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "MAX_LENGTH = 256  # ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ (ìƒ˜í”Œ ë°ì´í„°ì—ëŠ” 256ìœ¼ë¡œ ì¶©ë¶„)\n",
    "BATCH_SIZE = 4    # ë°°ì¹˜ í¬ê¸° (GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì •)\n",
    "LEARNING_RATE = 2e-5  # í•™ìŠµë¥ \n",
    "EPOCHS = 10       # ì—í­ ìˆ˜ (ìƒ˜í”Œ ë°ì´í„°ê°€ ì ìœ¼ë¯€ë¡œ ë” ë§ì´)\n",
    "DROPOUT = 0.3     # ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨\n",
    "TEST_SIZE = 0.2   # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¹„ìœ¨\n",
    "VAL_SIZE = 0.1    # ê²€ì¦ ë°ì´í„° ë¹„ìœ¨\n",
    "\n",
    "print(\"\\n=== í•˜ì´í¼íŒŒë¼ë¯¸í„° ===\")\n",
    "print(f\"ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´: {MAX_LENGTH}\")\n",
    "print(f\"ë°°ì¹˜ í¬ê¸°: {BATCH_SIZE}\")\n",
    "print(f\"í•™ìŠµë¥ : {LEARNING_RATE}\")\n",
    "print(f\"ì—í­ ìˆ˜: {EPOCHS}\")\n",
    "print(f\"ë“œë¡­ì•„ì›ƒ: {DROPOUT}\")\n",
    "print(f\"í…ŒìŠ¤íŠ¸/ê²€ì¦ ë¹„ìœ¨: {TEST_SIZE}/{VAL_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Val/Test ë¶„í• \n",
    "# ë°ì´í„°ê°€ ì ìœ¼ë¯€ë¡œ stratify ì‚¬ìš©í•˜ì—¬ í´ë˜ìŠ¤ ë¹„ìœ¨ ìœ ì§€\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df['text'].values,\n",
    "    df['label'].values,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=SEED,\n",
    "    stratify=df['label'].values\n",
    ")\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_texts,\n",
    "    train_labels,\n",
    "    test_size=VAL_SIZE/(1-TEST_SIZE),\n",
    "    random_state=SEED,\n",
    "    stratify=train_labels\n",
    ")\n",
    "\n",
    "print(\"\\n=== ë°ì´í„° ë¶„í•  ===\")\n",
    "print(f\"í•™ìŠµ ë°ì´í„°: {len(train_texts)}ê°œ\")\n",
    "print(f\"ê²€ì¦ ë°ì´í„°: {len(val_texts)}ê°œ\")\n",
    "print(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_texts)}ê°œ\")\n",
    "\n",
    "# Dataset ìƒì„±\n",
    "train_dataset = StanceDataset(train_texts, train_labels, tokenizer, MAX_LENGTH)\n",
    "val_dataset = StanceDataset(val_texts, val_labels, tokenizer, MAX_LENGTH)\n",
    "test_dataset = StanceDataset(test_texts, test_labels, tokenizer, MAX_LENGTH)\n",
    "\n",
    "# DataLoader ìƒì„±\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(\"\\nâœ… ë°ì´í„° ë¡œë” ìƒì„± ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ëª¨ë¸ ì •ì˜\n",
    "\n",
    "KoBERT ê¸°ë°˜ ìŠ¤íƒ ìŠ¤ ë¶„ë¥˜ ëª¨ë¸ì„ ì •ì˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StanceClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    KoBERT ê¸°ë°˜ ìŠ¤íƒ ìŠ¤ ë¶„ë¥˜ ëª¨ë¸\n",
    "    \n",
    "    Classes:\n",
    "        0: ì˜¹í˜¸ (Support)\n",
    "        1: ì¤‘ë¦½ (Neutral)\n",
    "        2: ë¹„íŒ (Oppose)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_classes=3, dropout=0.3):\n",
    "        super(StanceClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('skt/kobert-base-v1')\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        # [CLS] í† í°ì˜ ì¶œë ¥ ì‚¬ìš©\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# ëª¨ë¸ ì´ˆê¸°í™”\n",
    "print(\"ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘...\")\n",
    "model = StanceClassifier(n_classes=3, dropout=DROPOUT)\n",
    "model = model.to(device)\n",
    "\n",
    "# ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nâœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\")\n",
    "print(f\"   ì „ì²´ íŒŒë¼ë¯¸í„°: {total_params:,}\")\n",
    "print(f\"   í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. í•™ìŠµ\n",
    "\n",
    "ëª¨ë¸ì„ í•™ìŠµí•˜ê³  ê²€ì¦í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì € ì„¤ì •\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n\n# í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ (ì„ íƒì‚¬í•­)\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nscheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n\n# í•™ìŠµ ê¸°ë¡ ì €ì¥ìš©\nhistory = {\n    'train_loss': [],\n    'train_acc': [],\n    'val_loss': [],\n    'val_acc': []\n}\n\nprint(\"\\n=== í•™ìŠµ ì‹œì‘ ===\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµ í•¨ìˆ˜\n",
    "def train_epoch(model, data_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    progress_bar = tqdm(data_loader, desc='Training')\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ (ì„ íƒì‚¬í•­)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'acc': f'{100 * correct / total:.2f}%'\n",
    "        })\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# ê²€ì¦ í•¨ìˆ˜\n",
    "def eval_model(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc='Evaluating'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "print(\"âœ… í•™ìŠµ/ê²€ì¦ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµ ë£¨í”„\n",
    "best_val_acc = 0\n",
    "best_model_state = None\n",
    "patience_counter = 0\n",
    "max_patience = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # í•™ìŠµ\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    print(f\"\\nğŸ“Š Train Loss: {train_loss:.4f}, Train Acc: {train_acc*100:.2f}%\")\n",
    "    \n",
    "    # ê²€ì¦\n",
    "    val_loss, val_acc = eval_model(model, val_loader, criterion, device)\n",
    "    print(f\"ğŸ“Š Val Loss: {val_loss:.4f}, Val Acc: {val_acc*100:.2f}%\")\n",
    "    \n",
    "    # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # ê¸°ë¡ ì €ì¥\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_model_state = model.state_dict().copy()\n",
    "        patience_counter = 0\n",
    "        print(f\"\\nâ­ ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥! (Val Acc: {best_val_acc*100:.2f}%)\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= max_patience:\n",
    "            print(f\"\\nâš ï¸  Early stopping: {max_patience} ì—í­ ë™ì•ˆ ê°œì„  ì—†ìŒ\")\n",
    "            break\n",
    "\n",
    "print(f\"\\n\\n{'='*50}\")\n",
    "print(f\"ğŸ‰ í•™ìŠµ ì™„ë£Œ!\")\n",
    "print(f\"ìµœê³  ê²€ì¦ ì •í™•ë„: {best_val_acc*100:.2f}%\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "# ìµœê³  ì„±ëŠ¥ ëª¨ë¸ë¡œ ë³µì›\n",
    "if best_model_state:\n",
    "    model.load_state_dict(best_model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµ ê³¼ì • ì‹œê°í™”\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss ê·¸ë˜í”„\n",
    "ax1.plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "ax1.plot(history['val_loss'], label='Val Loss', marker='s')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy ê·¸ë˜í”„\n",
    "ax2.plot([acc*100 for acc in history['train_acc']], label='Train Acc', marker='o')\n",
    "ax2.plot([acc*100 for acc in history['val_acc']], label='Val Acc', marker='s')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.set_title('Training and Validation Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. í‰ê°€\n",
    "\n",
    "í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ìœ¼ë¡œ ëª¨ë¸ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„° í‰ê°€\n",
    "print(\"\\n=== í…ŒìŠ¤íŠ¸ í‰ê°€ ===\")\n",
    "test_loss, test_acc = eval_model(model, test_loader, criterion, device)\n",
    "print(f\"\\nğŸ“Š Test Loss: {test_loss:.4f}\")\n",
    "print(f\"ğŸ“Š Test Accuracy: {test_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒì„¸ ë¶„ë¥˜ ë³´ê³ ì„œ\n",
    "def get_predictions(model, data_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc='Getting predictions'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return np.array(predictions), np.array(true_labels)\n",
    "\n",
    "# ì˜ˆì¸¡ê°’ ê°€ì ¸ì˜¤ê¸°\n",
    "predictions, true_labels = get_predictions(model, test_loader, device)\n",
    "\n",
    "# ë¶„ë¥˜ ë³´ê³ ì„œ\n",
    "target_names = ['ì˜¹í˜¸ (Support)', 'ì¤‘ë¦½ (Neutral)', 'ë¹„íŒ (Oppose)']\n",
    "print(\"\\n=== ë¶„ë¥˜ ë³´ê³ ì„œ ===\")\n",
    "print(classification_report(true_labels, predictions, target_names=target_names, digits=4, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix ì‹œê°í™”\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Support', 'Neutral', 'Oppose'],\n",
    "            yticklabels=['Support', 'Neutral', 'Oppose'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ì •ê·œí™”ëœ Confusion Matrix\n",
    "cm_normalized = cm.astype('float') / (cm.sum(axis=1)[:, np.newaxis] + 1e-10)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues',\n",
    "            xticklabels=['Support', 'Neutral', 'Oppose'],\n",
    "            yticklabels=['Support', 'Neutral', 'Oppose'])\n",
    "plt.title('Normalized Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ëª¨ë¸ ì €ì¥\n",
    "\n",
    "í•™ìŠµëœ ëª¨ë¸ì„ ì €ì¥í•˜ê³  ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "save_dir = 'saved_models'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# ëª¨ë¸ ì €ì¥\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "model_filename = f'stance_classifier_{timestamp}.pth'\n",
    "model_path = os.path.join(save_dir, model_filename)\n",
    "\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"\\nâœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_path}\")\n",
    "\n",
    "# í•™ìŠµ ë©”íƒ€ë°ì´í„° ì €ì¥\n",
    "metadata = {\n",
    "    'timestamp': timestamp,\n",
    "    'test_accuracy': float(test_acc),\n",
    "    'test_loss': float(test_loss),\n",
    "    'best_val_accuracy': float(best_val_acc),\n",
    "    'hyperparameters': {\n",
    "        'max_length': MAX_LENGTH,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'epochs': EPOCHS,\n",
    "        'dropout': DROPOUT\n",
    "    },\n",
    "    'data_info': {\n",
    "        'total_samples': len(df),\n",
    "        'train_samples': len(train_texts),\n",
    "        'val_samples': len(val_texts),\n",
    "        'test_samples': len(test_texts)\n",
    "    },\n",
    "    'model_name': 'skt/kobert-base-v1'\n",
    "}\n",
    "\n",
    "metadata_filename = f'metadata_{timestamp}.json'\n",
    "metadata_path = os.path.join(save_dir, metadata_filename)\n",
    "\n",
    "with open(metadata_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"âœ… ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ: {metadata_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colabì—ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ\n",
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    \n",
    "    print(\"\\nğŸ“¥ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘...\")\n",
    "    files.download(model_path)\n",
    "    files.download(metadata_path)\n",
    "    print(\"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!\")\n",
    "else:\n",
    "    print(f\"\\nğŸ’¾ ë¡œì»¬ í™˜ê²½ì—ì„œëŠ” ë‹¤ìŒ ê²½ë¡œì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤:\")\n",
    "    print(f\"   ëª¨ë¸: {model_path}\")\n",
    "    print(f\"   ë©”íƒ€ë°ì´í„°: {metadata_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡\n",
    "\n",
    "ì €ì¥ëœ ëª¨ë¸ë¡œ ì‹¤ì œ ì˜ˆì¸¡ì„ í…ŒìŠ¤íŠ¸í•´ë´…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì˜ˆì¸¡ í•¨ìˆ˜\n",
    "def predict_stance(text, model, tokenizer, device, max_length=256):\n",
    "    model.eval()\n",
    "    \n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        probabilities = torch.softmax(outputs, dim=1)\n",
    "        confidence, predicted_class = torch.max(probabilities, dim=1)\n",
    "    \n",
    "    stance_labels = {0: \"ì˜¹í˜¸\", 1: \"ì¤‘ë¦½\", 2: \"ë¹„íŒ\"}\n",
    "    \n",
    "    return {\n",
    "        'stance': stance_labels[predicted_class.item()],\n",
    "        'stance_id': predicted_class.item(),\n",
    "        'confidence': round(confidence.item(), 4),\n",
    "        'probabilities': {\n",
    "            stance_labels[i]: round(prob, 4)\n",
    "            for i, prob in enumerate(probabilities[0].tolist())\n",
    "        }\n",
    "    }\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡\n",
    "test_texts_sample = [\n",
    "    \"ì´ë²ˆ ì •ì±…ì€ ê²½ì œ ë°œì „ì— í° ë„ì›€ì´ ë  ê²ƒìœ¼ë¡œ ê¸°ëŒ€ëœë‹¤. ì „ë¬¸ê°€ë“¤ë„ ê¸ì •ì ìœ¼ë¡œ í‰ê°€í•˜ê³  ìˆë‹¤.\",\n",
    "    \"ì •ë¶€ê°€ ìƒˆë¡œìš´ ë²•ì•ˆì„ ë°œí‘œí–ˆë‹¤. êµ¬ì²´ì ì¸ ì‹œí–‰ ë°©ì•ˆì€ ì¶”í›„ ê³µê°œë  ì˜ˆì •ì´ë‹¤.\",\n",
    "    \"ì´ë²ˆ ì¡°ì¹˜ëŠ” êµ­ë¯¼ì—ê²Œ ë¶€ë‹´ë§Œ ê°€ì¤‘ì‹œí‚¬ ë¿ì´ë‹¤. ì—¬ëŸ¬ ë¬¸ì œì ì´ ì§€ì ë˜ê³  ìˆë‹¤.\"\n",
    "]\n",
    "\n",
    "print(\"\\n=== ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸ ===\")\n",
    "for i, text in enumerate(test_texts_sample, 1):\n",
    "    result = predict_stance(text, model, tokenizer, device, MAX_LENGTH)\n",
    "    print(f\"\\n{i}. í…ìŠ¤íŠ¸: {text}\")\n",
    "    print(f\"   ì˜ˆì¸¡: {result['stance']} (ì‹ ë¢°ë„: {result['confidence']*100:.2f}%)\")\n",
    "    print(f\"   í™•ë¥ : ì˜¹í˜¸={result['probabilities']['ì˜¹í˜¸']:.4f}, \"\n",
    "          f\"ì¤‘ë¦½={result['probabilities']['ì¤‘ë¦½']:.4f}, \"\n",
    "          f\"ë¹„íŒ={result['probabilities']['ë¹„íŒ']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "### ì‹¤ì œ ë°ì´í„°ë¡œ í•™ìŠµí•˜ê¸°\n",
    "\n",
    "1. **ë°ì´í„° ì¤€ë¹„**\n",
    "   - `data/DATA_PREPARATION_GUIDE.md` ì°¸ê³ \n",
    "   - ìµœì†Œ í´ë˜ìŠ¤ë‹¹ 100ê°œ ì´ìƒ ê¶Œì¥\n",
    "   - CSV ë˜ëŠ” JSON í˜•ì‹ìœ¼ë¡œ ì¤€ë¹„\n",
    "\n",
    "2. **ë…¸íŠ¸ë¶ ì¬ì‹¤í–‰**\n",
    "   - \"ì˜µì…˜ 2: ì‹¤ì œ ë°ì´í„° ì—…ë¡œë“œ\" ì„¹ì…˜ì—ì„œ `USE_REAL_DATA = True`ë¡œ ë³€ê²½\n",
    "   - ë°ì´í„° ì—…ë¡œë“œ í›„ í•™ìŠµ ì§„í–‰\n",
    "\n",
    "3. **í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹**\n",
    "   - `MAX_LENGTH`: 512 (ê¸´ ê¸°ì‚¬ìš©)\n",
    "   - `BATCH_SIZE`: 8-16 (GPU ë©”ëª¨ë¦¬ì— ë”°ë¼)\n",
    "   - `EPOCHS`: 5-10\n",
    "   - `LEARNING_RATE`: 1e-5 ~ 5e-5\n",
    "\n",
    "4. **FastAPI ì„œë²„ í†µí•©**\n",
    "   - í•™ìŠµëœ ëª¨ë¸(.pth)ì„ `saved_models/` ë””ë ‰í† ë¦¬ì— ë³µì‚¬\n",
    "   - `.env` íŒŒì¼ì— ëª¨ë¸ ê²½ë¡œ ì„¤ì •\n",
    "   - API ì„œë²„ ì¬ì‹œì‘\n",
    "\n",
    "### ëª¨ë¸ ì„±ëŠ¥ ê°œì„  íŒ\n",
    "\n",
    "- ë°ì´í„° ì¦ê°• (Augmentation)\n",
    "- í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬ (Weighted Loss)\n",
    "- ì•™ìƒë¸” ëª¨ë¸\n",
    "- Fine-tuning ì „ëµ ë³€ê²½ (BERT layer freezing)\n",
    "\n",
    "### ë¬¸ì˜\n",
    "\n",
    "ë¬¸ì œê°€ ë°œìƒí•˜ë©´ GitHub Issuesì— ì˜¬ë ¤ì£¼ì„¸ìš”!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}