{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ìŠ¤íƒ ìŠ¤ ë¶„ì„ ëª¨ë¸ í•™ìŠµ (ê°œì„  ë²„ì „)\n",
    "\n",
    "KoBERT ê¸°ë°˜ ë‰´ìŠ¤ ê¸°ì‚¬ ìŠ¤íƒ ìŠ¤ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ\n",
    "- **3-class ë¶„ë¥˜**: ì˜¹í˜¸(0), ì¤‘ë¦½(1), ë¹„íŒ(2)\n",
    "- **ê°œì„ ì‚¬í•­**: Early Stopping, Weight Decay, Gradient Clipping, LR Scheduling\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab í™˜ê²½ í™•ì¸\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"âœ… Google Colab í™˜ê²½\")\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"âœ… ë¡œì»¬ í™˜ê²½\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜\n!pip install torch transformers scikit-learn pandas numpy matplotlib seaborn tqdm sentencepiece -q"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from transformers import BertModel, AutoTokenizer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ\")\n",
    "print(f\"PyTorch ë²„ì „: {torch.__version__}\")\n",
    "print(f\"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë””ë°”ì´ìŠ¤ ì„¤ì •\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • (ê°œì„  ë²„ì „)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# ê°œì„ ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„° (ê³¼ì í•© ë°©ì§€ ê°•í™”!)\n",
    "# ==========================================\n",
    "\n",
    "# ëª¨ë¸ íŒŒë¼ë¯¸í„°\n",
    "MAX_LENGTH = 128        # ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´\n",
    "DROPOUT = 0.4           # 0.3 â†’ 0.4ë¡œ ì¦ê°€ (ë” ê°•í•œ regularization)\n",
    "\n",
    "# í•™ìŠµ íŒŒë¼ë¯¸í„°\n",
    "BATCH_SIZE = 16         # ë°°ì¹˜ í¬ê¸°\n",
    "EPOCHS = 15             # ìµœëŒ€ epoch (early stoppingìœ¼ë¡œ ì¡°ê¸° ì¤‘ë‹¨)\n",
    "LEARNING_RATE = 2e-5    # 5e-5 â†’ 2e-5ë¡œ ë‚®ì¶¤ (ë” ì•ˆì •ì ì¸ í•™ìŠµ)\n",
    "WEIGHT_DECAY = 0.01     # Weight decay ì¶”ê°€! (L2 regularization)\n",
    "MAX_GRAD_NORM = 1.0     # Gradient clipping (exploding gradient ë°©ì§€)\n",
    "\n",
    "# Early Stopping íŒŒë¼ë¯¸í„°\n",
    "EARLY_STOP_PATIENCE = 3 # 3 epoch ë™ì•ˆ ê°œì„  ì—†ìœ¼ë©´ ì¤‘ë‹¨\n",
    "EARLY_STOP_MIN_DELTA = 0.001  # ê°œì„ ìœ¼ë¡œ ê°„ì£¼í•  ìµœì†Œ ë³€í™”ëŸ‰\n",
    "\n",
    "# ë°ì´í„° ë¶„í•  (ìƒ˜í”Œ ë°ì´í„° ì‚¬ìš© ì‹œ)\n",
    "TEST_SIZE = 0.1\n",
    "VAL_SIZE = 0.1\n",
    "\n",
    "# ì‹œë“œ ì„¤ì •\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(\"âœ… ê°œì„ ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • ì™„ë£Œ!\")\n",
    "print(f\"\\nğŸ“Š ì£¼ìš” ë³€ê²½ì‚¬í•­:\")\n",
    "print(f\"   - DROPOUT: 0.3 â†’ {DROPOUT}\")\n",
    "print(f\"   - LEARNING_RATE: 5e-5 â†’ {LEARNING_RATE}\")\n",
    "print(f\"   - WEIGHT_DECAY: 0 â†’ {WEIGHT_DECAY} (ìƒˆë¡œ ì¶”ê°€!)\")\n",
    "print(f\"   - MAX_GRAD_NORM: {MAX_GRAD_NORM} (ìƒˆë¡œ ì¶”ê°€!)\")\n",
    "print(f\"   - EARLY_STOP_PATIENCE: {EARLY_STOP_PATIENCE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ ë°ì´í„° ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ë¡œë“œ ë°©ì‹ ì„ íƒ\n",
    "USE_REAL_DATA = True  # True: ì‹¤ì œ CSV íŒŒì¼ ì‚¬ìš©, False: ìƒ˜í”Œ ë°ì´í„° ì‚¬ìš©\n",
    "\n",
    "if USE_REAL_DATA:\n",
    "    if IN_COLAB:\n",
    "        # Colab: íŒŒì¼ ì—…ë¡œë“œ\n",
    "        from google.colab import files\n",
    "        print(\"ğŸ“¤ ë°ì´í„° íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš” (train_dataset.csv, val_dataset.csv, test_dataset.csv)\")\n",
    "        uploaded = files.upload()\n",
    "        \n",
    "        train_df = pd.read_csv('train_dataset.csv')\n",
    "        val_df = pd.read_csv('val_dataset.csv')\n",
    "        test_df = pd.read_csv('test_dataset.csv')\n",
    "    else:\n",
    "        # ë¡œì»¬: íŒŒì¼ ê²½ë¡œ ì§€ì •\n",
    "        train_df = pd.read_csv('data/train_dataset.csv')\n",
    "        val_df = pd.read_csv('data/val_dataset.csv')\n",
    "        test_df = pd.read_csv('data/test_dataset.csv')\n",
    "    \n",
    "    print(f\"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ\")\n",
    "    print(f\"   - Train: {len(train_df)}ê°œ\")\n",
    "    print(f\"   - Val: {len(val_df)}ê°œ\")\n",
    "    print(f\"   - Test: {len(test_df)}ê°œ\")\n",
    "    \n",
    "else:\n",
    "    # ìƒ˜í”Œ ë°ì´í„° ìƒì„±\n",
    "    sample_data = {\n",
    "        'content': [\n",
    "            # ì˜¹í˜¸ (0)\n",
    "            \"ì •ë¶€ì˜ ìƒˆë¡œìš´ ì •ì±…ì€ ê²½ì œ í™œì„±í™”ì— í¬ê²Œ ê¸°ì—¬í•  ê²ƒìœ¼ë¡œ ê¸°ëŒ€ë©ë‹ˆë‹¤.\",\n",
    "            \"ì´ë²ˆ ê°œí˜ì•ˆì€ êµ­ë¯¼ë“¤ì˜ ì‚¶ì˜ ì§ˆ í–¥ìƒì— ë„ì›€ì´ ë  ê²ƒì…ë‹ˆë‹¤.\",\n",
    "            \"ëŒ€í†µë ¹ì˜ ê²°ì •ì€ êµ­ê°€ ë°œì „ì„ ìœ„í•œ ì˜¬ë°”ë¥¸ ë°©í–¥ì…ë‹ˆë‹¤.\",\n",
    "            \"ì •ë¶€ì˜ ì ê·¹ì ì¸ ëŒ€ì‘ì´ ìœ„ê¸° ê·¹ë³µì— íš¨ê³¼ì ì´ì—ˆìŠµë‹ˆë‹¤.\",\n",
    "            \"ìƒˆë¡œìš´ ë²•ì•ˆì€ ì‚¬íšŒ ì •ì˜ ì‹¤í˜„ì— ì¤‘ìš”í•œ ì—­í• ì„ í•  ê²ƒì…ë‹ˆë‹¤.\",\n",
    "            \"ì •ì±…ì˜ ê¸ì •ì  íš¨ê³¼ê°€ ê³³ê³³ì—ì„œ ë‚˜íƒ€ë‚˜ê³  ìˆìŠµë‹ˆë‹¤.\",\n",
    "            \"ì •ë¶€ì˜ ë…¸ë ¥ìœ¼ë¡œ ë§ì€ ë¬¸ì œê°€ í•´ê²°ë˜ê³  ìˆìŠµë‹ˆë‹¤.\",\n",
    "            \"ì´ë²ˆ ì¡°ì¹˜ëŠ” êµ­ë¯¼ì„ ìœ„í•œ í˜„ëª…í•œ ì„ íƒì´ì—ˆìŠµë‹ˆë‹¤.\",\n",
    "            # ì¤‘ë¦½ (1)\n",
    "            \"ì •ë¶€ê°€ ìƒˆë¡œìš´ ì •ì±…ì„ ë°œí‘œí–ˆìŠµë‹ˆë‹¤.\",\n",
    "            \"ì˜¤ëŠ˜ êµ­íšŒì—ì„œ ë²•ì•ˆì´ í†µê³¼ë˜ì—ˆìŠµë‹ˆë‹¤.\",\n",
    "            \"ëŒ€í†µë ¹ì´ ê¸°ìíšŒê²¬ì„ ì—´ì—ˆìŠµë‹ˆë‹¤.\",\n",
    "            \"ì •ë¶€ëŠ” ìƒˆë¡œìš´ ì˜ˆì‚°ì•ˆì„ ì œì¶œí–ˆìŠµë‹ˆë‹¤.\",\n",
    "            \"ê´€ë ¨ ë¶€ì²˜ê°€ ëŒ€ì±…ì„ ë…¼ì˜ ì¤‘ì…ë‹ˆë‹¤.\",\n",
    "            \"ì „ë¬¸ê°€ë“¤ì€ ë‹¤ì–‘í•œ ì˜ê²¬ì„ ì œì‹œí–ˆìŠµë‹ˆë‹¤.\",\n",
    "            \"ì •ì±… ì‹œí–‰ ì¼ì •ì´ ë°œí‘œë˜ì—ˆìŠµë‹ˆë‹¤.\",\n",
    "            \"ì—¬ì•¼ê°€ í˜‘ìƒì„ ì§„í–‰í•˜ê³  ìˆìŠµë‹ˆë‹¤.\",\n",
    "            # ë¹„íŒ (2)\n",
    "            \"ì •ë¶€ì˜ ì •ì±…ì€ í˜„ì‹¤ì„ ë°˜ì˜í•˜ì§€ ëª»í•˜ê³  ìˆìŠµë‹ˆë‹¤.\",\n",
    "            \"ì´ë²ˆ ê²°ì •ì€ êµ­ë¯¼ì˜ ëœ»ê³¼ ê±°ë¦¬ê°€ ë©‰ë‹ˆë‹¤.\",\n",
    "            \"ì •ì±…ì˜ ë¶€ì‘ìš©ì´ ìš°ë ¤ë©ë‹ˆë‹¤.\",\n",
    "            \"ì •ë¶€ì˜ ëŒ€ì‘ì´ ë¯¸í¡í•˜ë‹¤ëŠ” ë¹„íŒì´ ì œê¸°ë˜ê³  ìˆìŠµë‹ˆë‹¤.\",\n",
    "            \"ì „ë¬¸ê°€ë“¤ì€ ì´ë²ˆ ì¡°ì¹˜ì˜ íš¨ê³¼ì— ì˜ë¬¸ì„ í‘œí–ˆìŠµë‹ˆë‹¤.\",\n",
    "            \"ì •ì±… ì‹¤í–‰ ê³¼ì •ì—ì„œ ë§ì€ ë¬¸ì œì ì´ ë“œëŸ¬ë‚¬ìŠµë‹ˆë‹¤.\",\n",
    "            \"êµ­ë¯¼ë“¤ì˜ ë¶ˆë§Œì´ ë†’ì•„ì§€ê³  ìˆìŠµë‹ˆë‹¤.\",\n",
    "            \"ì´ë²ˆ ë²•ì•ˆì€ ì¬ê²€í† ê°€ í•„ìš”í•´ ë³´ì…ë‹ˆë‹¤.\"\n",
    "        ],\n",
    "        'label': [0]*8 + [1]*8 + [2]*8\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(sample_data)\n",
    "    \n",
    "    # ë°ì´í„° ë¶„í• \n",
    "    train_df, temp_df = train_test_split(df, test_size=0.2, random_state=SEED, stratify=df['label'])\n",
    "    val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=SEED, stratify=temp_df['label'])\n",
    "    \n",
    "    print(f\"âœ… ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì™„ë£Œ\")\n",
    "    print(f\"   - Train: {len(train_df)}ê°œ\")\n",
    "    print(f\"   - Val: {len(val_df)}ê°œ\")\n",
    "    print(f\"   - Test: {len(test_df)}ê°œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° í™•ì¸\n",
    "print(\"\\nğŸ“Š Train ë°ì´í„° ìƒ˜í”Œ:\")\n",
    "print(train_df.head())\n",
    "\n",
    "print(\"\\nğŸ“Š í´ë˜ìŠ¤ ë¶„í¬ (Train):\")\n",
    "label_counts = train_df['label'].value_counts().sort_index()\n",
    "label_names = ['ì˜¹í˜¸', 'ì¤‘ë¦½', 'ë¹„íŒ']\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"   {label} ({label_names[label]}): {count}ê°œ ({count/len(train_df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ ë°ì´í„° ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# KoBERT í† í¬ë‚˜ì´ì € ë¡œë“œ\n# monologg/kobert ì‚¬ìš© (ë” ì•ˆì •ì ì¸ í† í¬ë‚˜ì´ì§•)\nMODEL_NAME = 'monologg/kobert'\n\ntry:\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n    print(f\"âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ: {MODEL_NAME}\")\n    print(f\"   Vocab size: {len(tokenizer)}\")\n    \n    # í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸\n    test_text = \"ì •ë¶€ì˜ ìƒˆë¡œìš´ ì •ì±…ì´ ë°œí‘œë˜ì—ˆìŠµë‹ˆë‹¤.\"\n    test_encoded = tokenizer(test_text, return_tensors='pt')\n    test_decoded = tokenizer.decode(test_encoded['input_ids'][0], skip_special_tokens=False)\n    \n    print(f\"\\nâœ… í† í¬ë‚˜ì´ì € ë™ì‘ í™•ì¸:\")\n    print(f\"   ì›ë³¸: {test_text}\")\n    print(f\"   ë””ì½”ë”©: {test_decoded}\")\n    \n    # [UNK] í† í°ì´ ë§ìœ¼ë©´ ê²½ê³ \n    if test_decoded.count('[UNK]') > len(test_text) * 0.3:\n        print(\"âš ï¸  ê²½ê³ : [UNK] í† í°ì´ ë§ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ëª¨ë¸ì„ ì‹œë„í•˜ì„¸ìš”.\")\n    else:\n        print(\"âœ… ì •ìƒ ì‘ë™ ì¤‘\")\n        \nexcept Exception as e:\n    print(f\"âŒ í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨: {e}\")\n    print(\"\\nëŒ€ì²´ ë°©ë²• ì‹œë„: skt/kobert-base-v1\")\n    MODEL_NAME = 'skt/kobert-base-v1'\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n    print(f\"âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ: {MODEL_NAME}\")"
  },
  {
   "cell_type": "code",
   "source": "# í† í¬ë‚˜ì´ì € ìƒì„¸ í…ŒìŠ¤íŠ¸ (ì„ íƒ ì‚¬í•­)\n# ì‹¤ì œ ë‰´ìŠ¤ í…ìŠ¤íŠ¸ë¡œ í† í¬ë‚˜ì´ì € ë™ì‘ í™•ì¸\n\ntest_news = \"\"\"ì „ë¶ë„ì˜íšŒëŠ” 30ì¼ ì…ì¥ë¬¸ì„ ë‚´ê³  \"ì •ë¶€ê°€ í™•ì •í•œ ì œ4ì°¨ êµ­ê°€ì² ë„ë§ êµ¬ì¶•ê³„íšì€ ê´‘ì—­ê¶Œ ì—†ëŠ” ì „ë¶ì„ ì² ì €í•˜ê²Œ ì™¸ë©´í•œ ê²°ê³¼\"ë¼ë©° ê°•ë ¥í•˜ê²Œ í•­ì˜í–ˆë‹¤.\"\"\"\n\nprint(\"=\"*60)\nprint(\"í† í¬ë‚˜ì´ì € ìƒì„¸ í…ŒìŠ¤íŠ¸\")\nprint(\"=\"*60)\nprint(f\"\\nì›ë³¸ í…ìŠ¤íŠ¸:\\n{test_news}\\n\")\n\n# í† í¬ë‚˜ì´ì§•\nencoded = tokenizer(\n    test_news,\n    add_special_tokens=True,\n    max_length=128,\n    padding='max_length',\n    truncation=True,\n    return_tensors='pt'\n)\n\nprint(f\"Input IDs ê¸¸ì´: {len(encoded['input_ids'][0])}\")\nprint(f\"Attention Mask ê¸¸ì´: {len(encoded['attention_mask'][0])}\")\n\n# ë””ì½”ë”©\ndecoded_with_special = tokenizer.decode(encoded['input_ids'][0], skip_special_tokens=False)\ndecoded_without_special = tokenizer.decode(encoded['input_ids'][0], skip_special_tokens=True)\n\nprint(f\"\\në””ì½”ë”© ê²°ê³¼ (ìŠ¤í˜ì…œ í† í° í¬í•¨):\\n{decoded_with_special[:200]}...\")\nprint(f\"\\në””ì½”ë”© ê²°ê³¼ (ìŠ¤í˜ì…œ í† í° ì œì™¸):\\n{decoded_without_special[:200]}...\")\n\n# [UNK] ë¹„ìœ¨ ì²´í¬\nunk_count = decoded_with_special.count('[UNK]')\nunk_ratio = unk_count / len(test_news) if len(test_news) > 0 else 0\n\nprint(f\"\\n[UNK] í† í° ê°œìˆ˜: {unk_count}\")\nprint(f\"[UNK] ë¹„ìœ¨: {unk_ratio*100:.2f}%\")\n\nif unk_ratio > 0.3:\n    print(\"\\nâš ï¸  ê²½ê³ : [UNK] í† í° ë¹„ìœ¨ì´ ë†’ìŠµë‹ˆë‹¤ (30% ì´ìƒ)\")\n    print(\"   â†’ í† í¬ë‚˜ì´ì €ê°€ ì œëŒ€ë¡œ ì‘ë™í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n    print(\"   â†’ ë‹¤ë¥¸ ëª¨ë¸ì„ ì‹œë„í•˜ê±°ë‚˜ ì„¤ì¹˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\nelif unk_ratio > 0.1:\n    print(\"\\nâš ï¸  ì£¼ì˜: [UNK] í† í° ë¹„ìœ¨ì´ ë‹¤ì†Œ ë†’ìŠµë‹ˆë‹¤ (10% ì´ìƒ)\")\nelse:\n    print(\"\\nâœ… í† í¬ë‚˜ì´ì € ì •ìƒ ì‘ë™!\")\n    print(\"   â†’ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰í•˜ì„¸ìš”.\")\n\nprint(\"=\"*60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset í´ë˜ìŠ¤ ì •ì˜\n",
    "class StanceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    ìŠ¤íƒ ìŠ¤ ë¶„ë¥˜ë¥¼ ìœ„í•œ Dataset í´ë˜ìŠ¤\n",
    "    \"\"\"\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "print(\"âœ… StanceDataset í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader ìƒì„±\n",
    "train_texts = train_df['content'].tolist()\n",
    "train_labels = train_df['label'].tolist()\n",
    "\n",
    "val_texts = val_df['content'].tolist()\n",
    "val_labels = val_df['label'].tolist()\n",
    "\n",
    "test_texts = test_df['content'].tolist()\n",
    "test_labels = test_df['label'].tolist()\n",
    "\n",
    "train_dataset = StanceDataset(train_texts, train_labels, tokenizer, MAX_LENGTH)\n",
    "val_dataset = StanceDataset(val_texts, val_labels, tokenizer, MAX_LENGTH)\n",
    "test_dataset = StanceDataset(test_texts, test_labels, tokenizer, MAX_LENGTH)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(\"âœ… DataLoader ìƒì„± ì™„ë£Œ\")\n",
    "print(f\"   - Train batches: {len(train_loader)}\")\n",
    "print(f\"   - Val batches: {len(val_loader)}\")\n",
    "print(f\"   - Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ ëª¨ë¸ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìŠ¤íƒ ìŠ¤ ë¶„ë¥˜ ëª¨ë¸\n",
    "class StanceClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    KoBERT ê¸°ë°˜ ìŠ¤íƒ ìŠ¤ ë¶„ë¥˜ ëª¨ë¸\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name, num_classes=3, dropout=0.4):\n",
    "        super(StanceClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        # [CLS] í† í°ì˜ hidden state ì‚¬ìš©\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "print(\"âœ… StanceClassifier í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ì´ˆê¸°í™”\n",
    "model = StanceClassifier(\n",
    "    model_name=MODEL_NAME,\n",
    "    num_classes=3,\n",
    "    dropout=DROPOUT\n",
    ").to(device)\n",
    "\n",
    "# ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜ ì¶œë ¥\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"âœ… ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ\")\n",
    "print(f\"   - ì´ íŒŒë¼ë¯¸í„°: {total_params:,}\")\n",
    "print(f\"   - í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: {trainable_params:,}\")\n",
    "print(f\"   - Dropout: {DROPOUT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ Early Stopping í´ë˜ìŠ¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopping êµ¬í˜„\n",
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early Stopping: ê²€ì¦ ì†ì‹¤ì´ ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´ í•™ìŠµ ì¤‘ë‹¨\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=3, min_delta=0.001, verbose=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience: ê°œì„ ì´ ì—†ì–´ë„ ê¸°ë‹¤ë¦´ epoch ìˆ˜\n",
    "            min_delta: ê°œì„ ìœ¼ë¡œ ê°„ì£¼í•  ìµœì†Œ ë³€í™”ëŸ‰\n",
    "            verbose: ë¡œê·¸ ì¶œë ¥ ì—¬ë¶€\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        self.best_model_state = None\n",
    "    \n",
    "    def __call__(self, val_loss, model):\n",
    "        \"\"\"\n",
    "        ê²€ì¦ ì†ì‹¤ì„ ì²´í¬í•˜ê³  early stop ì—¬ë¶€ ê²°ì •\n",
    "        \"\"\"\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        \n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'âš ï¸  EarlyStopping counter: {self.counter}/{self.patience}')\n",
    "                print(f'   í˜„ì¬ Val Loss: {val_loss:.4f}, ìµœê³  Val Loss: {self.best_loss:.4f}')\n",
    "            \n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                if self.verbose:\n",
    "                    print(f'\\nğŸ›‘ Early Stopping! {self.patience} epoch ë™ì•ˆ ê°œì„  ì—†ìŒ')\n",
    "        \n",
    "        else:\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "    \n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        \"\"\"ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥\"\"\"\n",
    "        if self.verbose:\n",
    "            if self.best_loss is not None:\n",
    "                print(f'âœ… Val Loss ê°œì„ : {self.best_loss:.4f} â†’ {val_loss:.4f}')\n",
    "            else:\n",
    "                print(f'âœ… ìµœì´ˆ ëª¨ë¸ ì €ì¥ (Val Loss: {val_loss:.4f})')\n",
    "        \n",
    "        self.best_loss = val_loss\n",
    "        # ëª¨ë¸ state dict ì €ì¥ (ë©”ëª¨ë¦¬ì—)\n",
    "        self.best_model_state = {\n",
    "            key: value.cpu().clone() for key, value in model.state_dict().items()\n",
    "        }\n",
    "    \n",
    "    def load_best_model(self, model):\n",
    "        \"\"\"ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ\"\"\"\n",
    "        if self.best_model_state is not None:\n",
    "            model.load_state_dict(\n",
    "                {key: value.to(model.bert.device)\n",
    "                 for key, value in self.best_model_state.items()}\n",
    "            )\n",
    "            if self.verbose:\n",
    "                print(f'âœ… ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (Val Loss: {self.best_loss:.4f})')\n",
    "\n",
    "print(\"âœ… EarlyStopping í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7ï¸âƒ£ Optimizer ë° Scheduler ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Optimizer ë° Scheduler (ê°œì„ !)\n",
    "# ==========================================\n",
    "\n",
    "# í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚°\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(train_labels),\n",
    "    y=train_labels\n",
    ")\n",
    "class_weights = torch.FloatTensor(class_weights).to(device)\n",
    "\n",
    "print(\"\\ní´ë˜ìŠ¤ë³„ ê°€ì¤‘ì¹˜:\")\n",
    "for i, w in enumerate(class_weights):\n",
    "    label_name = ['ì˜¹í˜¸', 'ì¤‘ë¦½', 'ë¹„íŒ'][i]\n",
    "    print(f\"  {i} ({label_name}): {w:.4f}\")\n",
    "\n",
    "# Loss Function (í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# Optimizer (Weight Decay ì¶”ê°€!)\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,  # L2 regularization!\n",
    "    eps=1e-8\n",
    ")\n",
    "\n",
    "# Scheduler (ReduceLROnPlateau)\n",
    "scheduler = ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',        # validation lossë¥¼ ìµœì†Œí™”\n",
    "    factor=0.5,        # LRì„ ì ˆë°˜ìœ¼ë¡œ\n",
    "    patience=2,        # 2 epoch ë™ì•ˆ ê°œì„  ì—†ìœ¼ë©´ LR ê°ì†Œ\n",
    "    verbose=True,      # ë¡œê·¸ ì¶œë ¥\n",
    "    min_lr=1e-7        # ìµœì†Œ learning rate\n",
    ")\n",
    "\n",
    "# Early Stopping ì´ˆê¸°í™”\n",
    "early_stopping = EarlyStopping(\n",
    "    patience=EARLY_STOP_PATIENCE,\n",
    "    min_delta=EARLY_STOP_MIN_DELTA,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# í•™ìŠµ ê¸°ë¡ ì €ì¥ìš©\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': [],\n",
    "    'learning_rates': []  # LR ë³€í™” ì¶”ì \n",
    "}\n",
    "\n",
    "print(\"\\nâœ… Optimizer, Scheduler, Early Stopping ì„¤ì • ì™„ë£Œ!\")\n",
    "print(f\"   Weight Decay (L2): {WEIGHT_DECAY}\")\n",
    "print(f\"   Initial LR: {LEARNING_RATE}\")\n",
    "print(f\"   Early Stop Patience: {EARLY_STOP_PATIENCE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8ï¸âƒ£ í•™ìŠµ ë° ê²€ì¦ í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# í•™ìŠµ í•¨ìˆ˜ (Gradient Clipping ì¶”ê°€!)\n",
    "# ==========================================\n",
    "\n",
    "def train_epoch(model, data_loader, criterion, optimizer, device, max_grad_norm=1.0):\n",
    "    \"\"\"\n",
    "    1 epoch í•™ìŠµ\n",
    "    \n",
    "    Args:\n",
    "        max_grad_norm: gradient clipping ì„ê³„ê°’\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    progress_bar = tqdm(data_loader, desc='Training')\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient Clipping (ì¶”ê°€!)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # í†µê³„\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Progress bar ì—…ë°ì´íŠ¸\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'acc': f'{100 * correct / total:.2f}%'\n",
    "        })\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def eval_model(model, data_loader, criterion, device):\n",
    "    \"\"\"ëª¨ë¸ í‰ê°€ (ê²€ì¦/í…ŒìŠ¤íŠ¸)\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(data_loader, desc='Evaluating')\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{100 * correct / total:.2f}%'\n",
    "            })\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "print(\"âœ… ê°œì„ ëœ í•™ìŠµ/ê²€ì¦ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ!\")\n",
    "print(\"   - Gradient Clipping ì¶”ê°€\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9ï¸âƒ£ í•™ìŠµ ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# í•™ìŠµ ë£¨í”„ (Early Stopping ì ìš©!)\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"í•™ìŠµ ì‹œì‘ (Early Stopping í™œì„±í™”)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"ìµœëŒ€ Epoch: {EPOCHS}\")\n",
    "print(f\"Early Stop Patience: {EARLY_STOP_PATIENCE}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"Weight Decay: {WEIGHT_DECAY}\")\n",
    "print(f\"Dropout: {DROPOUT}\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "best_val_acc = 0.0\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # í˜„ì¬ Learning Rate ì¶œë ¥\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"ğŸ“Š Current Learning Rate: {current_lr:.2e}\")\n",
    "    \n",
    "    # í•™ìŠµ\n",
    "    train_loss, train_acc = train_epoch(\n",
    "        model, train_loader, criterion, optimizer, device,\n",
    "        max_grad_norm=MAX_GRAD_NORM\n",
    "    )\n",
    "    print(f\"\\nğŸ“Š Train Loss: {train_loss:.4f}, Train Acc: {train_acc*100:.2f}%\")\n",
    "    \n",
    "    # ê²€ì¦\n",
    "    val_loss, val_acc = eval_model(model, val_loader, criterion, device)\n",
    "    print(f\"ğŸ“Š Val Loss: {val_loss:.4f}, Val Acc: {val_acc*100:.2f}%\")\n",
    "    \n",
    "    # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # ê¸°ë¡ ì €ì¥\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['learning_rates'].append(current_lr)\n",
    "    \n",
    "    # ìµœê³  ì„±ëŠ¥ ì¶”ì \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_epoch = epoch + 1\n",
    "        print(f\"âœ¨ ìƒˆë¡œìš´ ìµœê³  ê²€ì¦ ì •í™•ë„: {best_val_acc*100:.2f}%\")\n",
    "    \n",
    "    # Early Stopping ì²´í¬\n",
    "    early_stopping(val_loss, model)\n",
    "    \n",
    "    if early_stopping.early_stop:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ğŸ›‘ Early Stopping at Epoch {epoch + 1}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        break\n",
    "\n",
    "# ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ\")\n",
    "print(f\"{'='*60}\")\n",
    "early_stopping.load_best_model(model)\n",
    "print(f\"âœ… Epoch {best_epoch}ì˜ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (Val Acc: {best_val_acc*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"í•™ìŠµ ì™„ë£Œ!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"ì´ í•™ìŠµ Epoch: {epoch + 1}/{EPOCHS}\")\n",
    "print(f\"ìµœê³  ê²€ì¦ ì •í™•ë„: {best_val_acc*100:.2f}% (Epoch {best_epoch})\")\n",
    "print(f\"ìµœì¢… Val Loss: {early_stopping.best_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”Ÿ í•™ìŠµ ê²°ê³¼ ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# í•™ìŠµ ê²°ê³¼ ì‹œê°í™” (Learning Rate ì¶”ê°€!)\n",
    "# ==========================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Loss ê·¸ë˜í”„\n",
    "axes[0].plot(history['train_loss'], 'o-', label='Train Loss', color='blue')\n",
    "axes[0].plot(history['val_loss'], 's-', label='Val Loss', color='orange')\n",
    "axes[0].axvline(x=best_epoch-1, color='red', linestyle='--', alpha=0.5,\n",
    "                label=f'Best (Epoch {best_epoch})')\n",
    "axes[0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy ê·¸ë˜í”„\n",
    "axes[1].plot([acc*100 for acc in history['train_acc']], 'o-',\n",
    "             label='Train Acc', color='blue')\n",
    "axes[1].plot([acc*100 for acc in history['val_acc']], 's-',\n",
    "             label='Val Acc', color='orange')\n",
    "axes[1].axvline(x=best_epoch-1, color='red', linestyle='--', alpha=0.5,\n",
    "                label=f'Best (Epoch {best_epoch})')\n",
    "axes[1].set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning Rate ë³€í™” (ìƒˆë¡œ ì¶”ê°€!)\n",
    "axes[2].plot(history['learning_rates'], 'o-', color='green')\n",
    "axes[2].set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('Learning Rate')\n",
    "axes[2].set_yscale('log')  # log scaleë¡œ ë³´ê¸°\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ìˆ˜ì¹˜ ìš”ì•½\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"í•™ìŠµ ìš”ì•½\")\n",
    "print(\"=\"*60)\n",
    "print(f\"ìµœê³  Train Acc: {max(history['train_acc'])*100:.2f}%\")\n",
    "print(f\"ìµœê³  Val Acc: {max(history['val_acc'])*100:.2f}% (Epoch {best_epoch})\")\n",
    "print(f\"ìµœì¢… Train Loss: {history['train_loss'][-1]:.4f}\")\n",
    "print(f\"ìµœì¢… Val Loss: {history['val_loss'][-1]:.4f}\")\n",
    "print(f\"Train/Val Acc ì°¨ì´: {(max(history['train_acc']) - max(history['val_acc']))*100:.2f}%p\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£1ï¸âƒ£ í…ŒìŠ¤íŠ¸ í‰ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„° í‰ê°€\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"í…ŒìŠ¤íŠ¸ ë°ì´í„° í‰ê°€\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_loss, test_acc = eval_model(model, test_loader, criterion, device)\n",
    "\n",
    "print(f\"\\nğŸ“Š Test Loss: {test_loss:.4f}\")\n",
    "print(f\"ğŸ“Š Test Accuracy: {test_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒì„¸ í‰ê°€ (Confusion Matrix & Classification Report)\n",
    "def get_predictions(model, data_loader, device):\n",
    "    \"\"\"ì˜ˆì¸¡ ê²°ê³¼ ìˆ˜ì§‘\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc='Predicting'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return np.array(all_preds), np.array(all_labels)\n",
    "\n",
    "# ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "test_preds, test_true = get_predictions(model, test_loader, device)\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Classification Report\")\n",
    "print(\"=\"*60)\n",
    "target_names = ['ì˜¹í˜¸', 'ì¤‘ë¦½', 'ë¹„íŒ']\n",
    "print(classification_report(test_true, test_preds, target_names=target_names))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(test_true, test_preds)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=target_names,\n",
    "            yticklabels=target_names)\n",
    "plt.title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£2ï¸âƒ£ ëª¨ë¸ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ì €ì¥\n",
    "MODEL_SAVE_PATH = 'stance_classifier_improved.pt'\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'best_val_acc': best_val_acc,\n",
    "    'best_epoch': best_epoch,\n",
    "    'history': history,\n",
    "    'hyperparameters': {\n",
    "        'MAX_LENGTH': MAX_LENGTH,\n",
    "        'DROPOUT': DROPOUT,\n",
    "        'BATCH_SIZE': BATCH_SIZE,\n",
    "        'LEARNING_RATE': LEARNING_RATE,\n",
    "        'WEIGHT_DECAY': WEIGHT_DECAY,\n",
    "        'MAX_GRAD_NORM': MAX_GRAD_NORM\n",
    "    }\n",
    "}, MODEL_SAVE_PATH)\n",
    "\n",
    "print(f\"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {MODEL_SAVE_PATH}\")\n",
    "\n",
    "# Colab í™˜ê²½ì—ì„œ ë‹¤ìš´ë¡œë“œ\n",
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    files.download(MODEL_SAVE_PATH)\n",
    "    print(\"ğŸ“¥ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£3ï¸âƒ£ ëª¨ë¸ ë¡œë“œ ë° í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ (ì˜ˆì‹œ)\n",
    "def load_model(model_path, device):\n",
    "    \"\"\"ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ\"\"\"\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    \n",
    "    # ëª¨ë¸ ì´ˆê¸°í™”\n",
    "    loaded_model = StanceClassifier(\n",
    "        model_name=MODEL_NAME,\n",
    "        num_classes=3,\n",
    "        dropout=checkpoint['hyperparameters']['DROPOUT']\n",
    "    ).to(device)\n",
    "    \n",
    "    # ê°€ì¤‘ì¹˜ ë¡œë“œ\n",
    "    loaded_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    print(f\"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")\n",
    "    print(f\"   - Best Val Acc: {checkpoint['best_val_acc']*100:.2f}%\")\n",
    "    print(f\"   - Best Epoch: {checkpoint['best_epoch']}\")\n",
    "    \n",
    "    return loaded_model\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ (ì£¼ì„ í•´ì œí•˜ì—¬ ì‚¬ìš©)\n",
    "# loaded_model = load_model(MODEL_SAVE_PATH, device)\n",
    "# test_loss, test_acc = eval_model(loaded_model, test_loader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹¤ì‹œê°„ ì˜ˆì¸¡ í•¨ìˆ˜\n",
    "def predict_stance(text, model, tokenizer, device, max_length=128):\n",
    "    \"\"\"\n",
    "    ë‹¨ì¼ í…ìŠ¤íŠ¸ì˜ ìŠ¤íƒ ìŠ¤ ì˜ˆì¸¡\n",
    "    \n",
    "    Args:\n",
    "        text: ì˜ˆì¸¡í•  ë‰´ìŠ¤ í…ìŠ¤íŠ¸\n",
    "        model: í•™ìŠµëœ ëª¨ë¸\n",
    "        tokenizer: í† í¬ë‚˜ì´ì €\n",
    "        device: ë””ë°”ì´ìŠ¤\n",
    "        max_length: ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´\n",
    "    \n",
    "    Returns:\n",
    "        predicted_label: ì˜ˆì¸¡ëœ ë¼ë²¨ (0, 1, 2)\n",
    "        predicted_class: ì˜ˆì¸¡ëœ í´ë˜ìŠ¤ëª…\n",
    "        probabilities: ê° í´ë˜ìŠ¤ë³„ í™•ë¥ \n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # í† í¬ë‚˜ì´ì§•\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    # ì˜ˆì¸¡\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        probabilities = torch.softmax(outputs, dim=1)\n",
    "        predicted_label = torch.argmax(probabilities, dim=1).item()\n",
    "    \n",
    "    label_names = ['ì˜¹í˜¸', 'ì¤‘ë¦½', 'ë¹„íŒ']\n",
    "    predicted_class = label_names[predicted_label]\n",
    "    \n",
    "    return predicted_label, predicted_class, probabilities.cpu().numpy()[0]\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ\n",
    "test_texts_sample = [\n",
    "    \"ì •ë¶€ì˜ ìƒˆë¡œìš´ ì •ì±…ì€ ê²½ì œ íšŒë³µì— í¬ê²Œ ê¸°ì—¬í•  ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤.\",\n",
    "    \"ì˜¤ëŠ˜ êµ­íšŒì—ì„œ ì˜ˆì‚°ì•ˆì´ í†µê³¼ë˜ì—ˆìŠµë‹ˆë‹¤.\",\n",
    "    \"ì´ë²ˆ ì •ì±…ì€ êµ­ë¯¼ì˜ ê¸°ëŒ€ì— ë¯¸ì¹˜ì§€ ëª»í•˜ê³  ìˆìŠµë‹ˆë‹¤.\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ì‹¤ì‹œê°„ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for text in test_texts_sample:\n",
    "    pred_label, pred_class, probs = predict_stance(text, model, tokenizer, device, MAX_LENGTH)\n",
    "    \n",
    "    print(f\"\\ní…ìŠ¤íŠ¸: {text}\")\n",
    "    print(f\"ì˜ˆì¸¡: {pred_class} (ë¼ë²¨ {pred_label})\")\n",
    "    print(f\"í™•ë¥ : ì˜¹í˜¸ {probs[0]:.2%}, ì¤‘ë¦½ {probs[1]:.2%}, ë¹„íŒ {probs[2]:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ ê°œì„ ì‚¬í•­ ìš”ì•½\n",
    "\n",
    "### âœ… ì ìš©ëœ ê°œì„ ì‚¬í•­\n",
    "\n",
    "1. **Early Stopping** (3 epoch patience)\n",
    "   - ê²€ì¦ ì†ì‹¤ì´ ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´ ìë™ ì¤‘ë‹¨\n",
    "   - ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ìë™ ì €ì¥ ë° ë³µì›\n",
    "\n",
    "2. **ì •ê·œí™” ê°•í™”**\n",
    "   - Dropout: 0.3 â†’ 0.4\n",
    "   - Weight Decay: 0.01 ì¶”ê°€ (L2 regularization)\n",
    "\n",
    "3. **í•™ìŠµ ì•ˆì •í™”**\n",
    "   - Learning Rate: 5e-5 â†’ 2e-5\n",
    "   - Gradient Clipping: 1.0 (exploding gradient ë°©ì§€)\n",
    "   - ReduceLROnPlateau ìŠ¤ì¼€ì¤„ëŸ¬\n",
    "\n",
    "4. **í–¥ìƒëœ ì‹œê°í™”**\n",
    "   - Learning Rate ë³€í™” ì¶”ì \n",
    "   - ìµœê³  ì„±ëŠ¥ epoch í‘œì‹œ\n",
    "   - ìƒì„¸í•œ í•™ìŠµ í†µê³„\n",
    "\n",
    "### ğŸš€ ì˜ˆìƒ íš¨ê³¼\n",
    "\n",
    "- **ê³¼ì í•© ì™„í™”**: Train/Val accuracy ì°¨ì´ ê°ì†Œ\n",
    "- **ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ**: í…ŒìŠ¤íŠ¸ ì •í™•ë„ ê°œì„ \n",
    "- **í•™ìŠµ íš¨ìœ¨ì„±**: ë¶ˆí•„ìš”í•œ epoch ë°©ì§€\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}